{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this file is to extract sentiment from the preprocessed, Reddit WSB comments which have been mapped to respective tickers. Sentiment analysis was performed using Vader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas vaderSentiment\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Add custom WallStreetBets words to the VADER lexicon\n",
    "# source https://github.com/jklepatch\n",
    "wsb_words = {\n",
    "    'citron': -4.0,\n",
    "    'hidenburg': -4.0,\n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,\n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,\n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "    'tendies': 2.0,\n",
    "    'town': 2.0,\n",
    "    'overvalued': -3.0,\n",
    "    'undervalued': 3.0,\n",
    "    'buy': 4.0,\n",
    "    'sell': -4.0,\n",
    "    'gone': -1.0,\n",
    "    'gtfo': -1.7,\n",
    "    'paper': -1.7,\n",
    "    'bullish': 3.7,\n",
    "    'bearish': -3.7,\n",
    "    'bagholder': -1.7,\n",
    "    'stonk': 1.9,\n",
    "    'green': 1.9,\n",
    "    'money': 1.2,\n",
    "    'print': 2.2,\n",
    "    'rocket': 2.2,\n",
    "    'bull': 2.9,\n",
    "    'bear': -2.9,\n",
    "    'pumping': -1.0,\n",
    "    'sus': -3.0,\n",
    "    'offering': -2.3,\n",
    "    'rip': -4.0,\n",
    "    'downgrade': -3.0,\n",
    "    'upgrade': 3.0,\n",
    "    'maintain': 1.0,\n",
    "    'pump': 1.9,\n",
    "    'hot': 1.5,\n",
    "    'drop': -2.5,\n",
    "    'rebound': 1.5,\n",
    "    'crack': 2.5,\n",
    "}\n",
    "\n",
    "# Update VADER lexicon with WSB words\n",
    "vader.lexicon.update(wsb_words)\n",
    "\n",
    "# Input and output directories - Google Colab was used, hence the filepaths \n",
    "input_dir = \"/content/drive/MyDrive/FYP/reddit part/ticker_csvs\"\n",
    "output_dir = \"/content/drive/MyDrive/FYP/reddit part/ticker_sentiment_csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        # Load the CSV file\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Ensure the 'body' column exists\n",
    "        if 'body' not in df.columns:\n",
    "            print(f\"Skipping {filename}: 'body' column not found.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate sentiment scores for each comment\n",
    "        df['sentiment_score'] = df['body'].apply(lambda x: vader.polarity_scores(str(x))['compound'])\n",
    "\n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        output_filepath = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Finished processing {filename}. Saved to {output_filepath}.\")\n",
    "\n",
    "print(f\"Sentiment analysis completed. Results saved in '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File paths\n",
    "articles_file = \"/content/drive/MyDrive/FYP/articles_with_tickers_with_monthly_avg_volume_v2.csv\"\n",
    "reddit_sentiment_dir = \"/content/drive/MyDrive/FYP/reddit part/ticker_sentiment_csvs\"\n",
    "output_file = \"/content/drive/MyDrive/FYP/articles_with_reddit_sentiment_all.csv\"\n",
    "\n",
    "# Load the articles dataset\n",
    "articles_df = pd.read_csv(articles_file)\n",
    "\n",
    "# Ensure 'reddit vader sentiment' column exists\n",
    "if 'reddit vader sentiment' not in articles_df.columns:\n",
    "    articles_df['reddit vader sentiment'] = ''\n",
    "\n",
    "# Load existing progress if output file exists\n",
    "if os.path.exists(output_file):\n",
    "    updated_articles_df = pd.read_csv(output_file)\n",
    "    processed_tickers = set(updated_articles_df['ticker'].unique())\n",
    "else:\n",
    "    updated_articles_df = articles_df.copy()\n",
    "    processed_tickers = set()\n",
    "\n",
    "# List all Reddit sentiment files\n",
    "reddit_files = [f for f in os.listdir(reddit_sentiment_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Process each Reddit CSV\n",
    "for reddit_file in tqdm(reddit_files, desc=\"Processing Reddit Sentiment Files\"):\n",
    "    # Extract the ticker from the filename\n",
    "    ticker = os.path.splitext(reddit_file)[0]\n",
    "\n",
    "    # Skip already processed tickers\n",
    "    if ticker in processed_tickers:\n",
    "        print(f\"Skipping already processed ticker: {ticker}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing ticker: {ticker}...\")\n",
    "\n",
    "    # Load the Reddit sentiment data for the ticker\n",
    "    reddit_df = pd.read_csv(os.path.join(reddit_sentiment_dir, reddit_file))\n",
    "\n",
    "    # Handle invalid dates in the Reddit dataset\n",
    "    def safe_parse_date(date):\n",
    "        try:\n",
    "            return pd.to_datetime(date, format='mixed', dayfirst=True).strftime('%d/%m/%Y')\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Parse and filter valid dates\n",
    "    reddit_df['Formatted_Date'] = reddit_df['comment_date'].apply(safe_parse_date)\n",
    "    reddit_df = reddit_df[reddit_df['Formatted_Date'].notna()]\n",
    "\n",
    "    # Filter articles for the current ticker\n",
    "    ticker_articles_df = articles_df[articles_df['ticker'] == ticker]\n",
    "\n",
    "    # Convert articles dates to match the same format\n",
    "    ticker_articles_df['Formatted_Date'] = pd.to_datetime(\n",
    "        ticker_articles_df['Formatted_Date'], format='mixed', dayfirst=True\n",
    "    ).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    # Find common dates\n",
    "    common_dates = set(ticker_articles_df['Formatted_Date']).intersection(set(reddit_df['Formatted_Date']))\n",
    "\n",
    "    # Filter Reddit dataset to keep only rows with common dates\n",
    "    reddit_common_df = reddit_df[reddit_df['Formatted_Date'].isin(common_dates)]\n",
    "\n",
    "    # Calculate the average sentiment score for each common date\n",
    "    average_sentiments = reddit_common_df.groupby('Formatted_Date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "    # Map the average sentiment scores to the articles dataset\n",
    "    ticker_articles_df['reddit vader sentiment'] = ticker_articles_df['Formatted_Date'].map(\n",
    "        average_sentiments.set_index('Formatted_Date')['sentiment_score']\n",
    "    )\n",
    "\n",
    "    # Update the main DataFrame\n",
    "    updated_articles_df.loc[updated_articles_df['ticker'] == ticker, 'reddit vader sentiment'] = ticker_articles_df[\n",
    "        'reddit vader sentiment'\n",
    "    ]\n",
    "\n",
    "    # Save progress after processing each ticker\n",
    "    updated_articles_df.to_csv(output_file, index=False)\n",
    "    print(f\"Progress saved for ticker: {ticker}\")\n",
    "\n",
    "print(f\"Sentiment analysis completed for all tickers. Final data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File path\n",
    "file_path = \"/content/drive/MyDrive/FYP/articles_with_reddit_sentiment_all.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter rows where both 'Sentiment_Score' and 'reddit vader sentiment' are not NaN\n",
    "filtered_df = df[df['reddit vader sentiment'].notna() & df['Sentiment_Score'].notna()]\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = filtered_df[['Sentiment_Score', 'reddit vader sentiment']].corr().iloc[0, 1]\n",
    "print(f\"Correlation between 'Sentiment_Score' and 'reddit vader sentiment': {correlation:.2f}\")\n",
    "\n",
    "# Scatter plot to visualize the relationship\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x='Sentiment_Score',\n",
    "    y='reddit vader sentiment',\n",
    "    data=filtered_df,\n",
    "    alpha=0.6,\n",
    "    edgecolor=None\n",
    ")\n",
    "plt.title(f\"Scatter Plot: Article Sentiment vs Reddit Sentiment\\nCorrelation = {correlation:.2f}\")\n",
    "plt.xlabel('Article Sentiment Score')\n",
    "plt.ylabel('Reddit Vader Sentiment')\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=0.7)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Density plot to compare sentiment distributions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(\n",
    "    filtered_df['Sentiment_Score'], label='Article Sentiment Score', shade=True\n",
    ")\n",
    "sns.kdeplot(\n",
    "    filtered_df['reddit vader sentiment'], label='Reddit Vader Sentiment', shade=True\n",
    ")\n",
    "plt.title(\"Density Plot: Article Sentiment vs Reddit Sentiment\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"/content/drive/MyDrive/FYP/articles_with_reddit_sentiment_all.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter rows where both 'Sentiment_Score' and 'reddit vader sentiment' are not NaN\n",
    "filtered_df = df[df['reddit vader sentiment'].notna() & df['Sentiment_Score'].notna()]\n",
    "\n",
    "# Define conditions\n",
    "positive_article_positive_reddit = (\n",
    "    (filtered_df['Sentiment_Score'] > 0) & (filtered_df['reddit vader sentiment'] > 0)\n",
    ")\n",
    "positive_article_negative_reddit = (\n",
    "    (filtered_df['Sentiment_Score'] > 0) & (filtered_df['reddit vader sentiment'] < 0)\n",
    ")\n",
    "negative_article_positive_reddit = (\n",
    "    (filtered_df['Sentiment_Score'] < 0) & (filtered_df['reddit vader sentiment'] > 0)\n",
    ")\n",
    "negative_article_negative_reddit = (\n",
    "    (filtered_df['Sentiment_Score'] < 0) & (filtered_df['reddit vader sentiment'] < 0)\n",
    ")\n",
    "\n",
    "# Count cases\n",
    "count_positive_positive = positive_article_positive_reddit.sum()\n",
    "count_positive_negative = positive_article_negative_reddit.sum()\n",
    "count_negative_positive = negative_article_positive_reddit.sum()\n",
    "count_negative_negative = negative_article_negative_reddit.sum()\n",
    "\n",
    "# Print results\n",
    "print(\"Number of cases:\")\n",
    "print(f\"+ve Article Sentiment and +ve Reddit Sentiment: {count_positive_positive}\")\n",
    "print(f\"+ve Article Sentiment and -ve Reddit Sentiment: {count_positive_negative}\")\n",
    "print(f\"-ve Article Sentiment and +ve Reddit Sentiment: {count_negative_positive}\")\n",
    "print(f\"-ve Article Sentiment and -ve Reddit Sentiment: {count_negative_negative}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
