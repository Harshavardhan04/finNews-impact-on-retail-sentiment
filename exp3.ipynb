{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna torch tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Load the dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/cleaned_final.csv\")\n",
    "\n",
    "# 2) Basic checks\n",
    "required_columns = [\"Sentiment_Score\", \"reddit vader sentiment\", \"Options % Spike\"]\n",
    "for c in required_columns:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing {c} in CSV\")\n",
    "\n",
    "# 3) Drop rows with missing values\n",
    "df.dropna(subset=required_columns, inplace=True)\n",
    "\n",
    "# Additional features? (Optional)\n",
    "# feature_cols = [\"Sentiment_Score\",\"some_other_feature1\",\"some_other_feature2\",...]\n",
    "feature_cols = [\"Sentiment_Score\"]  # minimal example using only news sentiment\n",
    "\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Now define the 2-output target: [y1, y2]\n",
    "# y1 = 'reddit vader sentiment'\n",
    "# y2 = 'Options % Spike'\n",
    "y = df[[\"reddit vader sentiment\",\"Options % Spike\"]].values.astype(np.float32)\n",
    "\n",
    "# 4) Train/val/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test     = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 5) Scaling input features (optional but typical)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# If desired, scale the outputs too:\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# y_scaler = MinMaxScaler()\n",
    "# y_train  = y_scaler.fit_transform(y_train)\n",
    "# y_val    = y_scaler.transform(y_val)\n",
    "# y_test   = y_scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiOutputDataset(X_train_t, y_train_t)\n",
    "val_dataset   = MultiOutputDataset(X_val_t,   y_val_t)\n",
    "test_dataset  = MultiOutputDataset(X_test_t,  y_test_t)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        # final layer with 2 outputs:\n",
    "        layers.append(nn.Linear(in_dim, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "# or custom weighted approach:\n",
    "# e.g. Weighted MSE or sum of separate MSE for each output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def create_model(trial, input_dim):\n",
    "    \"\"\"Builds the model using trial parameters.\"\"\"\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "\n",
    "    model = MultiOutputModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Create model with hyperparams\n",
    "    model = create_model(trial, input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "    # Suggest learning rate\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    # Suggest weight decay\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Train for a few epochs (can adjust)\n",
    "    EPOCHS = 15\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_x)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_preds = model(val_x)\n",
    "                v_loss = criterion(val_preds, val_y)\n",
    "                val_loss += v_loss.item() * val_x.size(0)\n",
    "\n",
    "        # Early stopping or partial usage:\n",
    "        average_val_loss = val_loss / len(val_dataset)\n",
    "        # We can tell Optuna about the intermediate result:\n",
    "        trial.report(average_val_loss, epoch)\n",
    "\n",
    "        # Could do early stop if not improving\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return average_val_loss\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# RUN OPTUNA STUDY\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # or more trials for thorough search\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "print(\"Best hyperparameters found by Optuna:\", best_params)\n",
    "\n",
    "# Rebuild the model with best hyperparams\n",
    "best_model = MultiOutputModel(\n",
    "    input_dim = X_train.shape[1],\n",
    "    hidden_dim = best_params[\"hidden_dim\"],\n",
    "    num_layers = best_params[\"num_layers\"],\n",
    "    dropout = best_params[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(best_model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optionally merge train & val for final training\n",
    "X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "trainval_dataset = MultiOutputDataset(\n",
    "    torch.tensor(X_trainval,dtype=torch.float32),\n",
    "    torch.tensor(y_trainval,dtype=torch.float32)\n",
    ")\n",
    "trainval_loader = DataLoader(trainval_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# Train final model\n",
    "EPOCHS_FINAL = 200\n",
    "for epoch in range(1, EPOCHS_FINAL+1):\n",
    "    best_model.train()\n",
    "    train_loss = 0.0\n",
    "    for bx, by in trainval_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = best_model(bx)\n",
    "        loss = criterion(out, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * bx.size(0)\n",
    "    print(f\"Epoch {epoch}: TrainLoss={(train_loss/len(trainval_dataset)):.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "preds_list, truth_list = [], []\n",
    "with torch.no_grad():\n",
    "    for tx, ty in test_loader:\n",
    "        tx = tx.to(device)\n",
    "        outputs = best_model(tx)\n",
    "        preds_list.append(outputs.cpu().numpy())\n",
    "        truth_list.append(ty.numpy())\n",
    "\n",
    "preds_arr = np.concatenate(preds_list, axis=0)  # shape: [num_samples, 2]\n",
    "truth_arr = np.concatenate(truth_list, axis=0)  # shape: [num_samples, 2]\n",
    "\n",
    "mse = np.mean((preds_arr - truth_arr)**2)\n",
    "print(f\"Final Test MSE: {mse:.4f}\")\n",
    "\n",
    "# Optionally compute correlation for each output dimension:\n",
    "from scipy.stats import pearsonr\n",
    "corr_reddit = pearsonr(preds_arr[:,0], truth_arr[:,0])[0]\n",
    "corr_options = pearsonr(preds_arr[:,1], truth_arr[:,1])[0]\n",
    "print(f\"Correlation for Reddit Sentiment: {corr_reddit:.4f}\")\n",
    "print(f\"Correlation for Options Spike:    {corr_options:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "def fetch_trading_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches trading data for a given ticker and date range, and processes column names.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing processed trading data with 'Date', 'close_today', and 'close_prev_day' columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching trading data for ticker: {ticker}, Range: {start_date} to {end_date}\")\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "        # Reset the index to make 'Date' a column\n",
    "        stock_data.reset_index(inplace=True)\n",
    "\n",
    "        # Flatten any MultiIndex columns (in case they occur)\n",
    "        stock_data.columns = [\n",
    "            col[0] if isinstance(col, tuple) else col for col in stock_data.columns\n",
    "        ]\n",
    "\n",
    "        # Ensure the 'Date' column is datetime\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "        # Add the previous trading day's close\n",
    "        stock_data['close_prev_day'] = stock_data['Close'].shift(1)\n",
    "\n",
    "        # Rename 'Close' to 'close_today'\n",
    "        stock_data.rename(columns={'Close': 'close_today'}, inplace=True)\n",
    "\n",
    "        # Add ticker column for merging\n",
    "        stock_data['ticker'] = ticker\n",
    "\n",
    "        return stock_data[['Date', 'ticker', 'close_today', 'close_prev_day']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching trading data for {ticker}: {e}\")\n",
    "        return pd.DataFrame(columns=['Date', 'ticker', 'close_today', 'close_prev_day'])\n",
    "\n",
    "def process_ticker_data(ticker, cleaned_final, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch and process trading data for a single ticker, and merge it with the original dataset.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol.\n",
    "        cleaned_final (pd.DataFrame): Original dataset with 'ticker' and 'Formatted_Date' columns.\n",
    "        start_date (str): Start date for fetching data.\n",
    "        end_date (str): End date for fetching data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with 'close_today' and 'close_prev_day' columns merged.\n",
    "    \"\"\"\n",
    "    # Ensure 'ticker' column exists in cleaned_final\n",
    "    if 'ticker' not in cleaned_final.columns:\n",
    "        raise KeyError(f\"'ticker' column is missing in the input DataFrame.\")\n",
    "\n",
    "    # Fetch trading data for the ticker\n",
    "    stock_data = fetch_trading_data(ticker, start_date, end_date)\n",
    "\n",
    "    # Debugging: Print structure of both DataFrames\n",
    "    print(f\"Structure of cleaned_final before merge for ticker {ticker}:\")\n",
    "    print(cleaned_final.info())\n",
    "    print(cleaned_final.head())\n",
    "\n",
    "    print(f\"Structure of stock_data for ticker {ticker}:\")\n",
    "    print(stock_data.info())\n",
    "    print(stock_data.head())\n",
    "\n",
    "    # Merge the trading data with the cleaned_final DataFrame\n",
    "    cleaned_final = pd.merge(\n",
    "        cleaned_final,\n",
    "        stock_data,\n",
    "        left_on=['Formatted_Date', 'ticker'],\n",
    "        right_on=['Date', 'ticker'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop the redundant 'Date' column after the merge\n",
    "    cleaned_final.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    return cleaned_final\n",
    "\n",
    "# Example usage\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "\n",
    "# Replace this with your actual cleaned_final DataFrame\n",
    "# Ensure 'Formatted_Date' is in datetime format and 'ticker' is present\n",
    "cleaned_final = pd.DataFrame({\n",
    "    'Formatted_Date': pd.to_datetime(['2023-01-03', '2023-01-04', '2023-01-05']),\n",
    "    'ticker': ['AAPL', 'AAPL', 'AAPL']\n",
    "})\n",
    "\n",
    "# Process a specific ticker\n",
    "ticker = \"AAPL\"\n",
    "try:\n",
    "    cleaned_final = process_ticker_data(ticker, cleaned_final, start_date, end_date)\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Debugging: Inspect the final merged DataFrame\n",
    "print(\"Final cleaned_final after merge:\")\n",
    "print(cleaned_final.info())\n",
    "print(cleaned_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the Formatted_Date column is in datetime format\n",
    "cleaned_final['Formatted_Date'] = pd.to_datetime(cleaned_final['Formatted_Date'], format='mixed')\n",
    "\n",
    "# Get the unique tickers and date range\n",
    "unique_tickers = cleaned_final['ticker'].unique()\n",
    "start_date = cleaned_final['Formatted_Date'].min()\n",
    "end_date = cleaned_final['Formatted_Date'].max()\n",
    "\n",
    "# Define a function to fetch trading data\n",
    "def fetch_trading_data(ticker, start_date, end_date):\n",
    "    try:\n",
    "        print(f\"Fetching trading data for ticker: {ticker}, Range: {start_date} to {end_date}\")\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        if not stock_data.empty:\n",
    "            stock_data.reset_index(inplace=True)\n",
    "            stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "            return stock_data[['Date', 'Close']]\n",
    "        else:\n",
    "            print(f\"No trading data found for {ticker} in the specified range.\")\n",
    "            return pd.DataFrame(columns=['Date', 'Close'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching trading data for {ticker}: {e}\")\n",
    "        return pd.DataFrame(columns=['Date', 'Close'])\n",
    "\n",
    "# Create a DataFrame to hold all tickers' trading data\n",
    "all_trading_data = pd.DataFrame()\n",
    "\n",
    "# Fetch trading data for all tickers\n",
    "for ticker in unique_tickers:\n",
    "    ticker_data = fetch_trading_data(ticker, start_date, end_date)\n",
    "    if not ticker_data.empty:\n",
    "        ticker_data['ticker'] = ticker\n",
    "        all_trading_data = pd.concat([all_trading_data, ticker_data], ignore_index=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "all_trading_data.rename(columns={'Date': 'Formatted_Date', 'Close': 'close_today'}, inplace=True)\n",
    "\n",
    "# Sort trading data by ticker and date\n",
    "all_trading_data.sort_values(by=['ticker', 'Formatted_Date'], inplace=True)\n",
    "\n",
    "# Add the close_prev_day column\n",
    "all_trading_data['close_prev_day'] = (\n",
    "    all_trading_data.groupby('ticker')['close_today']\n",
    "    .shift(1)\n",
    "    .reset_index(drop=True)  # Fix to avoid multi-column errors\n",
    ")\n",
    "\n",
    "# Merge the trading data back into the cleaned_final DataFrame\n",
    "merged_data = pd.merge(\n",
    "    cleaned_final,\n",
    "    all_trading_data,\n",
    "    on=['ticker', 'Formatted_Date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_data.to_csv(\"cleaned_final_with_closing_prices.csv\", index=False)\n",
    "print(\"Data successfully processed and saved to 'cleaned_final_with_closing_prices.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Load the existing dataset\n",
    "cleaned_final = pd.read_csv(\"/content/drive/MyDrive/FYP/cleaned_final.csv\")\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "cleaned_final['Formatted_Date'] = pd.to_datetime(cleaned_final['Formatted_Date'], format='mixed')\n",
    "\n",
    "# Filter for Apple (AAPL) only\n",
    "aapl_data = cleaned_final[cleaned_final['ticker'] == 'AAPL'].copy()\n",
    "\n",
    "# Fetch Apple historical stock data\n",
    "start_date = aapl_data['Formatted_Date'].min()\n",
    "end_date = aapl_data['Formatted_Date'].max()\n",
    "\n",
    "# Download data from Yahoo Finance\n",
    "aapl_stock_data = yf.download(\"AAPL\", start=start_date, end=end_date)\n",
    "aapl_stock_data.reset_index(inplace=True)\n",
    "\n",
    "# Flatten MultiIndex columns in aapl_stock_data\n",
    "aapl_stock_data.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) and col[1] else col[0]\n",
    "    for col in aapl_stock_data.columns\n",
    "]\n",
    "\n",
    "# Debugging: Print column names after flattening\n",
    "print(\"Flattened columns in aapl_stock_data:\", aapl_stock_data.columns)\n",
    "\n",
    "# Rename 'Close_AAPL' to 'close_today' for clarity\n",
    "aapl_stock_data.rename(columns={'Close_AAPL': 'close_today'}, inplace=True)\n",
    "\n",
    "# Add previous day's close\n",
    "aapl_stock_data['close_prev_day'] = aapl_stock_data['close_today'].shift(1)\n",
    "\n",
    "# Debugging: Check the structure of aapl_stock_data after processing\n",
    "print(aapl_stock_data.head())\n",
    "\n",
    "# Merge the new data back into aapl_data\n",
    "aapl_data = pd.merge(\n",
    "    aapl_data,\n",
    "    aapl_stock_data[['Date', 'close_today', 'close_prev_day']],\n",
    "    left_on='Formatted_Date',\n",
    "    right_on='Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra Date column from the merge\n",
    "#aapl_data.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "# Save the updated data to a new CSV\n",
    "output_path = \"aapl_with_closing_prices.csv\"  # Replace with desired output path\n",
    "aapl_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated AAPL data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Load the existing dataset\n",
    "cleaned_final = pd.read_csv(\"/content/drive/MyDrive/FYP/cleaned_final.csv\")\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "cleaned_final['Formatted_Date'] = pd.to_datetime(cleaned_final['Formatted_Date'], format='mixed')\n",
    "\n",
    "# Collect processed data from each ticker\n",
    "processed_data = []\n",
    "\n",
    "# Get the unique tickers\n",
    "tickers = cleaned_final['ticker'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "\n",
    "    # 1. Filter rows for the current ticker\n",
    "    ticker_data = cleaned_final[cleaned_final['ticker'] == ticker].copy()\n",
    "\n",
    "    # 2. Figure out the date range for that ticker\n",
    "    start_date = ticker_data['Formatted_Date'].min()\n",
    "    end_date = ticker_data['Formatted_Date'].max()\n",
    "\n",
    "    # 3. Download historical data from Yahoo Finance\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "\n",
    "    # 4. Flatten any MultiIndex columns (in case yfinance returned them)\n",
    "    stock_data.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) and col[1] else col[0]\n",
    "        for col in stock_data.columns\n",
    "    ]\n",
    "    print(f\"Flattened columns in {ticker} stock data:\", stock_data.columns)\n",
    "\n",
    "    # 5. Rename 'Close_<ticker>' to 'close_today' to match your single‐ticker logic\n",
    "    old_close_col = f\"Close_{ticker}\"\n",
    "    if old_close_col in stock_data.columns:\n",
    "        stock_data.rename(columns={old_close_col: 'close_today'}, inplace=True)\n",
    "    else:\n",
    "        # If, for some reason, the exact name 'Close_<ticker>' doesn't exist, you can decide what to do:\n",
    "        print(f\"Warning: '{old_close_col}' not found. Check whether yfinance data matches the expected column naming.\")\n",
    "        # Fallback is optional; if you truly need it to be 'Close_<ticker>', just continue or raise an error.\n",
    "\n",
    "    # 6. Add the previous day's close\n",
    "    if 'close_today' in stock_data.columns:\n",
    "        stock_data['close_prev_day'] = stock_data['close_today'].shift(1)\n",
    "    else:\n",
    "        stock_data['close_prev_day'] = None\n",
    "\n",
    "    # 7. Merge this back into your ticker_data on the matching dates\n",
    "    ticker_data = pd.merge(\n",
    "        ticker_data,\n",
    "        stock_data[['Date', 'close_today', 'close_prev_day']],\n",
    "        left_on='Formatted_Date',\n",
    "        right_on='Date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 8. Drop the extra 'Date' column, if desired\n",
    "    # In your original code it’s commented out, so leave it that way if you want the same logic:\n",
    "    # ticker_data.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    # 9. Accumulate the result\n",
    "    processed_data.append(ticker_data)\n",
    "\n",
    "# 10. Concatenate all ticker slices into one final DataFrame\n",
    "final_data = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "# 11. Save the merged dataset to a single CSV\n",
    "output_path = \"/content/drive/MyDrive/FYP/all_tickers_with_closing_prices.csv\"\n",
    "final_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Done! Combined data with close/prev_close columns is in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_data=final_data.dropna()\n",
    "output_path =  \"/content/drive/MyDrive/FYP/all_tickers_with_closing_prices.csv\"\n",
    "final_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = final_data\n",
    "\n",
    "\n",
    "data.dropna(subset=['close_today', 'close_prev_day'], inplace=True)\n",
    "\n",
    "# Calculate the momentum\n",
    "data['momentum'] = ((data['close_today'] - data['close_prev_day']) / data['close_prev_day']) * 100\n",
    "\n",
    "# Save the updated DataFrame to a new CSV (optional)\n",
    "output_path = \"data_with_momentum.csv\"  # Replace with your desired file path\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Momentum calculated and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redoing the Model with both momentum and news sentiment, striving for better correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install optuna torch tqdm scikit-learn\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Ensure the needed columns exist\n",
    "required_cols = [\"Sentiment_Score\", \"reddit vader sentiment\", \"Options % Spike\", \"momentum\"]\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "# Drop any rows that have NaN in these columns\n",
    "df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "# Prepare features (NOW including momentum)\n",
    "feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Prepare multi-output target: [reddit_sentiment, options_spike]\n",
    "y = df[[\"reddit vader sentiment\", \"Options % Spike\"]].values.astype(np.float32)\n",
    "\n",
    "# Train / Val / Test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test     = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# Optionally, one could also scale the outputs if desired, e.g., MinMaxScaler.\n",
    "# For simplicity, leaving y unscaled here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n",
    "\n",
    "train_dataset = MultiOutputDataset(X_train_t, y_train_t)\n",
    "val_dataset   = MultiOutputDataset(X_val_t,   y_val_t)\n",
    "test_dataset  = MultiOutputDataset(X_test_t,  y_test_t)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        # Final layer: outputs 2 values for 2-target regression\n",
    "        layers.append(nn.Linear(in_dim, 2))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def create_model(trial, input_dim):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "\n",
    "    model = MultiOutputModel(\n",
    "        input_dim  = input_dim,\n",
    "        hidden_dim = hidden_dim,\n",
    "        num_layers = num_layers,\n",
    "        dropout    = dropout\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial, input_dim=len(feature_cols))\n",
    "\n",
    "    # Suggest learning rate and weight decay\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    EPOCHS = 15  # can adjust\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_x)\n",
    "            loss = criterion(preds, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_preds = model(val_x)\n",
    "                v_loss = criterion(val_preds, val_y)\n",
    "                val_loss += v_loss.item() * val_x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataset)\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        # Optional pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Increase n_trials for a more exhaustive search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "print(\"Best hyperparameters found by Optuna:\", best_params)\n",
    "\n",
    "# Rebuild model with best hyperparams\n",
    "best_model = MultiOutputModel(\n",
    "    input_dim=len(feature_cols),\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    dropout=best_params[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    best_model.parameters(),\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Combine train + val for final training if you like\n",
    "X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "trainval_dataset = MultiOutputDataset(\n",
    "    torch.tensor(X_trainval, dtype=torch.float32),\n",
    "    torch.tensor(y_trainval, dtype=torch.float32)\n",
    ")\n",
    "trainval_loader = DataLoader(trainval_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "EPOCHS_FINAL = 20\n",
    "for epoch in range(1, EPOCHS_FINAL+1):\n",
    "    best_model.train()\n",
    "    total_loss = 0.0\n",
    "    for bx, by in trainval_loader:\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = best_model(bx)\n",
    "        loss = criterion(out, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * bx.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(trainval_dataset)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS_FINAL} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Final training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "preds_list = []\n",
    "truth_list = []\n",
    "with torch.no_grad():\n",
    "    for tx, ty in test_loader:\n",
    "        tx = tx.to(device)\n",
    "        outputs = best_model(tx)\n",
    "        preds_list.append(outputs.cpu().numpy())\n",
    "        truth_list.append(ty.numpy())\n",
    "\n",
    "preds_arr = np.concatenate(preds_list, axis=0)  # shape [N, 2]\n",
    "truth_arr = np.concatenate(truth_list, axis=0)  # shape [N, 2]\n",
    "\n",
    "mse = np.mean((preds_arr - truth_arr)**2)\n",
    "print(f\"Final Test MSE (joint): {mse:.4f}\")\n",
    "\n",
    "# Evaluate correlation for each output dimension\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# preds_arr[:,0] vs. truth_arr[:,0] => correlation for reddit sentiment\n",
    "corr_reddit = pearsonr(preds_arr[:,0], truth_arr[:,0])[0]\n",
    "# preds_arr[:,1] vs. truth_arr[:,1] => correlation for options spike\n",
    "corr_options = pearsonr(preds_arr[:,1], truth_arr[:,1])[0]\n",
    "\n",
    "print(f\"Correlation (Reddit sentiment): {corr_reddit:.4f}\")\n",
    "print(f\"Correlation (Options spike):    {corr_options:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models split by sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example ticker-to-sector map as provided\n",
    "ticker_to_sector = {\n",
    "    # Communication Services\n",
    "    'DIS': 'Communication Services', 'NFLX': 'Communication Services', 'CMCSA': 'Communication Services',\n",
    "    'TMUS': 'Communication Services', 'GOOGL': 'Communication Services', 'CHTR': 'Communication Services',\n",
    "\n",
    "    # Consumer Discretionary\n",
    "    'AMZN': 'Consumer Discretionary', 'TSLA': 'Consumer Discretionary', 'NKE': 'Consumer Discretionary',\n",
    "    'HD': 'Consumer Discretionary', 'AZO': 'Consumer Discretionary', 'YUM': 'Consumer Discretionary',\n",
    "\n",
    "    # Consumer Staples\n",
    "    'WMT': 'Consumer Staples', 'KO': 'Consumer Staples', 'PEP': 'Consumer Staples', 'MDLZ': 'Consumer Staples',\n",
    "    'PG': 'Consumer Staples', 'KHC': 'Consumer Staples', 'MO': 'Consumer Staples',\n",
    "\n",
    "    # Energy\n",
    "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'NEM': 'Energy',\n",
    "\n",
    "    # Financials\n",
    "    'BAC': 'Financials', 'JPM': 'Financials', 'C': 'Financials', 'MA': 'Financials', 'V': 'Financials',\n",
    "    'WFC': 'Financials', 'CB': 'Financials',\n",
    "\n",
    "    # Health Care\n",
    "    'JNJ': 'Health Care', 'PFE': 'Health Care', 'ABBV': 'Health Care', 'MRK': 'Health Care',\n",
    "    'LLY': 'Health Care', 'GILD': 'Health Care', 'MDT': 'Health Care', 'UNH': 'Health Care',\n",
    "\n",
    "    # Industrials\n",
    "    'GE': 'Industrials', 'BA': 'Industrials', 'UPS': 'Industrials', 'MMM': 'Industrials',\n",
    "    'DELL': 'Industrials',\n",
    "\n",
    "    # Information Technology\n",
    "    'AAPL': 'Information Technology', 'MSFT': 'Information Technology', 'NVDA': 'Information Technology',\n",
    "    'QCOM': 'Information Technology', 'ADBE': 'Information Technology', 'INTC': 'Information Technology',\n",
    "    'CSCO': 'Information Technology', 'TXN': 'Information Technology', 'IBM': 'Information Technology',\n",
    "    'ORCL': 'Information Technology', 'PYPL': 'Information Technology', 'AVGO': 'Information Technology',\n",
    "    'AMD': 'Information Technology', 'ADP': 'Information Technology', 'INTU': 'Information Technology',\n",
    "\n",
    "    # Materials\n",
    "    'LIN': 'Materials', 'DHR': 'Materials',\n",
    "\n",
    "    # Real Estate\n",
    "    'PSA': 'Real Estate',\n",
    "\n",
    "    # Utilities\n",
    "    'DUK': 'Utilities', 'ED': 'Utilities', 'NEE': 'Utilities', 'PPL': 'Utilities',\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Add 'sector' column\n",
    "df['sector'] = df['ticker'].map(ticker_to_sector)\n",
    "\n",
    "# Filter out any rows with missing sector or missing numeric columns\n",
    "required_cols = ['Sentiment_Score','reddit vader sentiment','Options % Spike','momentum','sector']\n",
    "df.dropna(subset=required_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        # final layer output = 2\n",
    "        layers.append(nn.Linear(in_dim, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_for_sector(df_sector, feature_cols, device, epochs=20, batch_size=256):\n",
    "    \"\"\"\n",
    "    Trains a multi-output model on the data for one sector.\n",
    "    Returns (model, scaler, X_val, y_val, X_test, y_test).\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract input features X and multi-output Y\n",
    "    X = df_sector[feature_cols].values.astype(np.float32)\n",
    "    y = df_sector[[\"reddit vader sentiment\",\"Options % Spike\"]].values.astype(np.float32)\n",
    "\n",
    "    # Train/Val split (80/20). For more robust approach, do 70/15/15 etc.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "    # Scale input features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val   = scaler.transform(X_val)\n",
    "\n",
    "    # Convert to torch\n",
    "    X_train_t = torch.tensor(X_train,dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train,dtype=torch.float32)\n",
    "    X_val_t   = torch.tensor(X_val,  dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,  dtype=torch.float32)\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = MultiOutputDataset(X_train_t, y_train_t)\n",
    "    val_ds   = MultiOutputDataset(X_val_t,   y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Build model (hard-coded hyperparams or define your own logic)\n",
    "    input_dim= len(feature_cols)\n",
    "    model = MultiOutputModel(input_dim=input_dim, hidden_dim=128, num_layers=3, dropout=0.3).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(bx)\n",
    "            loss = criterion(preds, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * bx.size(0)\n",
    "        train_mse = total_loss / len(train_ds)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for vx, vy in val_loader:\n",
    "                vx, vy = vx.to(device), vy.to(device)\n",
    "                vpreds = model(vx)\n",
    "                vloss  = criterion(vpreds, vy)\n",
    "                val_loss += vloss.item() * vx.size(0)\n",
    "        val_mse = val_loss / len(val_ds)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] TrainMSE={train_mse:.4f}  ValMSE={val_mse:.4f}\")\n",
    "\n",
    "    # Return everything needed\n",
    "    return model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Choose the feature columns (including momentum)\n",
    "feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "\n",
    "# We can identify the unique sectors present in the DataFrame\n",
    "unique_sectors = df['sector'].dropna().unique()\n",
    "\n",
    "# Dictionary to hold the results\n",
    "sector_models = {}\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "for sec in unique_sectors:\n",
    "    print(f\"\\nTraining sector '{sec}'\")\n",
    "    df_sector = df[df['sector'] == sec].copy()\n",
    "    if len(df_sector) < 100:\n",
    "        print(f\"  Not enough data in sector {sec}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    model, scaler = train_model_for_sector(\n",
    "        df_sector=df_sector,\n",
    "        feature_cols=feature_cols,\n",
    "        device=device,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Store the model + scaler in a dict\n",
    "    sector_models[sec] = {\n",
    "        'model': model,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "print(\"\\nAll sectors processed. Models stored in sector_models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sector_model(df_sector_test, feature_cols, model, scaler, device):\n",
    "    \"\"\" Evaluate an already-trained sector model on a sector's test set.\n",
    "        Returns MSE, correlation for both outputs, etc.\n",
    "    \"\"\"\n",
    "    if len(df_sector_test) == 0:\n",
    "        return None\n",
    "\n",
    "    X_test = df_sector_test[feature_cols].values.astype(np.float32)\n",
    "    y_test = df_sector_test[[\"reddit vader sentiment\",\"Options % Spike\"]].values.astype(np.float32)\n",
    "\n",
    "    # Scale X\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truths= []\n",
    "    with torch.no_grad():\n",
    "        batch_size = 256\n",
    "        for i in range(0, len(X_test_t), batch_size):\n",
    "            bx = X_test_t[i:i+batch_size].to(device)\n",
    "            by = y_test_t[i:i+batch_size]\n",
    "            out = model(bx).cpu().numpy()  # shape (N,2)\n",
    "            preds.append(out)\n",
    "            truths.append(by.numpy())\n",
    "    preds_arr = np.concatenate(preds, axis=0)\n",
    "    truth_arr = np.concatenate(truths, axis=0)\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = np.mean((preds_arr - truth_arr)**2)\n",
    "\n",
    "    # Correlations for each dimension\n",
    "    from scipy.stats import pearsonr\n",
    "    corr_reddit = pearsonr(preds_arr[:,0], truth_arr[:,0])[0]\n",
    "    corr_options= pearsonr(preds_arr[:,1], truth_arr[:,1])[0]\n",
    "\n",
    "    return mse, corr_reddit, corr_options\n",
    "\n",
    "# Example usage, if you had a separate df_test for each sector\n",
    "# for sec, items in sector_models.items():\n",
    "#     df_sect_test = df_test[df_test[\"sector\"]==sec]\n",
    "#     res = evaluate_sector_model(df_sect_test, feature_cols, items['model'], items['scaler'], device)\n",
    "#     if res is not None:\n",
    "#         print(sec, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "################################################################################\n",
    "# 1) Ticker-to-Sector Mapping\n",
    "################################################################################\n",
    "ticker_to_sector = {\n",
    "    # Communication Services\n",
    "    'DIS': 'Communication Services', 'NFLX': 'Communication Services', 'CMCSA': 'Communication Services',\n",
    "    'TMUS': 'Communication Services', 'GOOGL': 'Communication Services', 'CHTR': 'Communication Services',\n",
    "\n",
    "    # Consumer Discretionary\n",
    "    'AMZN': 'Consumer Discretionary', 'TSLA': 'Consumer Discretionary', 'NKE': 'Consumer Discretionary',\n",
    "    'HD': 'Consumer Discretionary', 'AZO': 'Consumer Discretionary', 'YUM': 'Consumer Discretionary',\n",
    "\n",
    "    # Consumer Staples\n",
    "    'WMT': 'Consumer Staples', 'KO': 'Consumer Staples', 'PEP': 'Consumer Staples', 'MDLZ': 'Consumer Staples',\n",
    "    'PG': 'Consumer Staples', 'KHC': 'Consumer Staples', 'MO': 'Consumer Staples',\n",
    "\n",
    "    # Energy\n",
    "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'NEM': 'Energy',\n",
    "\n",
    "    # Financials\n",
    "    'BAC': 'Financials', 'JPM': 'Financials', 'C': 'Financials', 'MA': 'Financials', 'V': 'Financials',\n",
    "    'WFC': 'Financials', 'CB': 'Financials',\n",
    "\n",
    "    # Health Care\n",
    "    'JNJ': 'Health Care', 'PFE': 'Health Care', 'ABBV': 'Health Care', 'MRK': 'Health Care',\n",
    "    'LLY': 'Health Care', 'GILD': 'Health Care', 'MDT': 'Health Care', 'UNH': 'Health Care',\n",
    "\n",
    "    # Industrials\n",
    "    'GE': 'Industrials', 'BA': 'Industrials', 'UPS': 'Industrials', 'MMM': 'Industrials',\n",
    "    'DELL': 'Industrials',\n",
    "\n",
    "    # Information Technology\n",
    "    'AAPL': 'Information Technology', 'MSFT': 'Information Technology', 'NVDA': 'Information Technology',\n",
    "    'QCOM': 'Information Technology', 'ADBE': 'Information Technology', 'INTC': 'Information Technology',\n",
    "    'CSCO': 'Information Technology', 'TXN': 'Information Technology', 'IBM': 'Information Technology',\n",
    "    'ORCL': 'Information Technology', 'PYPL': 'Information Technology', 'AVGO': 'Information Technology',\n",
    "    'AMD': 'Information Technology', 'ADP': 'Information Technology', 'INTU': 'Information Technology',\n",
    "\n",
    "    # Materials\n",
    "    'LIN': 'Materials', 'DHR': 'Materials',\n",
    "\n",
    "    # Real Estate\n",
    "    'PSA': 'Real Estate',\n",
    "\n",
    "    # Utilities\n",
    "    'DUK': 'Utilities', 'ED': 'Utilities', 'NEE': 'Utilities', 'PPL': 'Utilities',\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "# 2) Load the CSV and Map Tickers to Sectors\n",
    "################################################################################\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Create sector column\n",
    "df['sector'] = df['ticker'].map(ticker_to_sector)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols = ['ticker','Sentiment_Score','reddit vader sentiment','Options % Spike','momentum','sector']\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "# Drop rows with missing values in these columns\n",
    "df.dropna(subset=required_cols, inplace=True)\n",
    "\n",
    "# (Optional) If some rows have no mapped sector, they become NaN in 'sector'\n",
    "df = df.dropna(subset=['sector'])\n",
    "\n",
    "################################################################################\n",
    "# 3) Dataset & Multi-Output Model Definitions\n",
    "################################################################################\n",
    "class MultiOutputDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        # final layer has 2 outputs: [RedditSent, OptionsSpike]\n",
    "        layers.append(nn.Linear(in_dim, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "################################################################################\n",
    "# 4) Train/Eval Function for One Sector\n",
    "################################################################################\n",
    "def train_and_evaluate_sector(df_sector, feature_cols, device, epochs=20, batch_size=256):\n",
    "    \"\"\"\n",
    "    Splits the sector's data into train/val (80/20), trains a multi-output model,\n",
    "    evaluates on val set, and returns final stats + the trained model + scaler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare inputs (X) and outputs (y)\n",
    "    X = df_sector[feature_cols].values.astype(np.float32)\n",
    "    y = df_sector[[\"reddit vader sentiment\",\"Options % Spike\"]].values.astype(np.float32)\n",
    "\n",
    "    # Train/Val split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val   = scaler.transform(X_val)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.float32)\n",
    "\n",
    "    train_ds = MultiOutputDataset(X_train_t, y_train_t)\n",
    "    val_ds   = MultiOutputDataset(X_val_t,   y_val_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Build model\n",
    "    input_dim = len(feature_cols)\n",
    "    model = MultiOutputModel(input_dim=input_dim, hidden_dim=128, num_layers=3, dropout=0.3).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    # Train Loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(bx)\n",
    "            loss = criterion(preds, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item() * bx.size(0)\n",
    "        train_mse = train_loss_sum / len(train_ds)\n",
    "\n",
    "        # Validation pass\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        preds_val_list = []\n",
    "        truth_val_list = []\n",
    "        with torch.no_grad():\n",
    "            for vx, vy in val_loader:\n",
    "                vx = vx.to(device)\n",
    "                vy = vy.to(device)\n",
    "                vpreds = model(vx)\n",
    "                vloss  = criterion(vpreds, vy)\n",
    "                val_loss_sum += vloss.item() * vx.size(0)\n",
    "                preds_val_list.append(vpreds.cpu().numpy())\n",
    "                truth_val_list.append(vy.cpu().numpy())\n",
    "        val_mse = val_loss_sum / len(val_ds)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}: Train MSE={train_mse:.4f}, Val MSE={val_mse:.4f}\")\n",
    "\n",
    "    # Final evaluation on val set: correlation\n",
    "    preds_arr = np.concatenate(preds_val_list, axis=0)  # shape [N,2]\n",
    "    truth_arr = np.concatenate(truth_val_list, axis=0)  # shape [N,2]\n",
    "\n",
    "    # MSE\n",
    "    final_mse = np.mean((preds_arr - truth_arr)**2)\n",
    "    # Pearson correlations\n",
    "    corr_reddit   = pearsonr(preds_arr[:,0], truth_arr[:,0])[0] if len(preds_arr)>1 else 0\n",
    "    corr_options  = pearsonr(preds_arr[:,1], truth_arr[:,1])[0] if len(preds_arr)>1 else 0\n",
    "\n",
    "    print(f\"[Final Validation Results] MSE={final_mse:.4f}, Reddit Corr={corr_reddit:.4f}, Options Corr={corr_options:.4f}\")\n",
    "\n",
    "    # Return\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"val_mse\": final_mse,\n",
    "        \"val_corr_reddit\": corr_reddit,\n",
    "        \"val_corr_options\": corr_options,\n",
    "    }\n",
    "\n",
    "################################################################################\n",
    "# 5) Sector-Wise Loop\n",
    "################################################################################\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Features (including momentum)\n",
    "    feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "\n",
    "    # Identify unique sectors\n",
    "    unique_sectors = df[\"sector\"].unique()\n",
    "\n",
    "    # Dictionary to store models/results\n",
    "    sector_models = {}\n",
    "\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    for sector_name in unique_sectors:\n",
    "        df_sector = df[df[\"sector\"] == sector_name]\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"Training sector: {sector_name}\")\n",
    "        print(f\"Rows in sector: {len(df_sector)}\")\n",
    "\n",
    "        if len(df_sector) < 50:\n",
    "            print(\"Not enough data; skipping.\")\n",
    "            continue\n",
    "\n",
    "        results = train_and_evaluate_sector(\n",
    "            df_sector      = df_sector,\n",
    "            feature_cols   = feature_cols,\n",
    "            device         = device,\n",
    "            epochs         = EPOCHS,\n",
    "            batch_size     = BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        sector_models[sector_name] = results\n",
    "\n",
    "    print(\"\\nAll sectors processed. Here is a summary:\\n\")\n",
    "    for sec, res in sector_models.items():\n",
    "        print(f\"S E C T O R: {sec}\")\n",
    "        print(f\"  Val MSE:          {res['val_mse']:.4f}\")\n",
    "        print(f\"  Corr (Reddit):    {res['val_corr_reddit']:.4f}\")\n",
    "        print(f\"  Corr (Options):   {res['val_corr_options']:.4f}\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install pandas numpy scikit-learn xgboost matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define RSI using weighted sum of Options % Spike and Reddit Vader Sentiment\n",
    "df[\"RSI\"] = (0.8 * df[\"Options % Spike\"]) + (0.2 * df[\"reddit vader sentiment\"])\n",
    "\n",
    "# Convert RSI into a binary classification target\n",
    "df[\"RSI_label\"] = np.where(df[\"RSI\"] > 0, 1, 0)\n",
    "\n",
    "# Print class distribution\n",
    "print(df[\"RSI_label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define input features and target\n",
    "feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "X = df[feature_cols]\n",
    "y = df[\"RSI_label\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (80-20 stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n--- {model_name} Model Evaluation ---\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative RSI\", \"Positive RSI\"], yticklabels=[\"Negative RSI\", \"Positive RSI\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate all models\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\n",
    "evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
    "evaluate_model(\"XGBoost\", y_test, y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for model, y_pred, name in zip([lr, rf, xgb], [y_pred_lr, y_pred_rf, y_pred_xgb], [\"Logistic Regression\", \"Random Forest\", \"XGBoost\"]):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]):.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Random baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-AUC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define MLP Model\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "y_pred_mlp = (mlp_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"MLP Accuracy: {acc_mlp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries if not already installed\n",
    "%pip install tpot --upgrade\n",
    "\n",
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Load dataset (Replace with actual dataset path)\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_cols = [\"momentum\", \"Sentiment_Score\", \"Options % Spike\", \"reddit vader sentiment\"]\n",
    "assert all(col in df.columns for col in required_cols), \"Missing required columns!\"\n",
    "\n",
    "# Scale down 'Options % Spike' before RSI computation\n",
    "df[\"Scaled_Options_Spike\"] = df[\"Options % Spike\"] / 10  # Adjust scaling factor if needed\n",
    "\n",
    "# Compute RSI using a weighted formula\n",
    "df[\"RSI_value\"] = 0.7 * df[\"Scaled_Options_Spike\"] + 0.3 * df[\"reddit vader sentiment\"]\n",
    "\n",
    "# Convert RSI into a binary classification (1 if RSI > 0, else 0)\n",
    "df[\"RSI_label\"] = np.where(df[\"RSI_value\"] > 0, 1, 0)\n",
    "\n",
    "# Define input features for classification model (momentum and sentiment score)\n",
    "feature_cols = [\"momentum\", \"Sentiment_Score\"]\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"RSI_label\"]\n",
    ")\n",
    "\n",
    "# Define feature matrices and target vectors\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[\"RSI_label\"]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[\"RSI_label\"]\n",
    "\n",
    "# Initialize and Train the TPOT Classifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=10,\n",
    "    population_size=50,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Best Model Found by TPOT\n",
    "y_pred_tpot = tpot.predict(X_test)\n",
    "acc_tpot = accuracy_score(y_test, y_pred_tpot)\n",
    "print(f\"AutoML TPOT Accuracy: {acc_tpot:.4f}\")\n",
    "\n",
    "# Display Classification Report\n",
    "print(\"Classification Report for TPOT Model:\")\n",
    "print(classification_report(y_test, y_pred_tpot))\n",
    "\n",
    "# Save Best Model Pipeline\n",
    "tpot.export(\"best_model_pipeline.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define RSI using weighted sum of Options % Spike and Reddit Vader Sentiment\n",
    "df[\"RSI\"] = (0.8 * df[\"Options % Spike\"]) + (0.2 * df[\"reddit vader sentiment\"])\n",
    "\n",
    "# Convert RSI into a binary classification target\n",
    "df[\"RSI_label\"] = np.where(df[\"RSI\"] > 0, 1, 0)\n",
    "\n",
    "# Print class distribution\n",
    "print(df[\"RSI_label\"].value_counts(normalize=True))\n",
    "\n",
    "# Define input features and target\n",
    "feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "X = df[feature_cols]\n",
    "y = df[\"RSI_label\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)  # Convert back to DataFrame for interpretability\n",
    "\n",
    "# Train-test split (80-20 stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize models\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "\n",
    "# Train models\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.4f}\")\n",
    "\n",
    "# Function to plot feature importance\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    if hasattr(model, \"feature_importances_\"):  # Works for Random Forest\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(x=importances[indices], y=np.array(feature_names)[indices], palette=\"Blues_r\")\n",
    "        plt.xlabel(\"Feature Importance Score\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        plt.title(f\"{model_name} Feature Importance\")\n",
    "        plt.show()\n",
    "\n",
    "# Feature Importance for Random Forest\n",
    "plot_feature_importance(rf, feature_cols, \"Random Forest\")\n",
    "\n",
    "# Feature Importance for XGBoost\n",
    "def plot_xgb_importance(model, feature_names):\n",
    "    importance_dict = model.get_booster().get_score(importance_type='weight')\n",
    "    importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": [importance_dict.get(f\"f{i}\", 0) for i in range(len(feature_names))]})\n",
    "    importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=importance_df[\"Importance\"], y=importance_df[\"Feature\"], palette=\"Oranges_r\")\n",
    "    plt.xlabel(\"Feature Importance Score\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.title(\"XGBoost Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "plot_xgb_importance(xgb, feature_cols)\n",
    "\n",
    "# SHAP Feature Importance (For Explainability)\n",
    "def plot_shap(model, X_sample, model_name):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_sample)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"{model_name} SHAP Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# SHAP for XGBoost (More Detailed Feature Importance)\n",
    "plot_shap(xgb, X_train, \"XGBoost\")\n",
    "\n",
    "# Model Evaluation Function\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n--- {model_name} Model Evaluation ---\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative RSI\", \"Positive RSI\"], yticklabels=[\"Negative RSI\", \"Positive RSI\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate all models\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\n",
    "evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
    "evaluate_model(\"XGBoost\", y_test, y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/FYP/data_with_momentum.csv\")\n",
    "df = df.dropna()  # Drop missing values\n",
    "\n",
    "# Define RSI using weighted sum of Options % Spike and Reddit Vader Sentiment\n",
    "df[\"RSI\"] = (0.8 * df[\"Options % Spike\"]) + (0.2 * df[\"reddit vader sentiment\"])\n",
    "\n",
    "# Convert RSI into a binary classification target\n",
    "df[\"RSI_label\"] = np.where(df[\"RSI\"] > 0, 1, 0)\n",
    "\n",
    "# Define input features and target\n",
    "feature_cols = [\"Sentiment_Score\", \"momentum\"]\n",
    "X = df[feature_cols]\n",
    "y = df[\"RSI_label\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)  # Convert back to DataFrame for SHAP\n",
    "\n",
    "# Train-test split (80-20 stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize models\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "\n",
    "# Train models\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {acc_rf:.4f}\")\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.4f}\")\n",
    "\n",
    "# Train MLP Model\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Evaluate MLP\n",
    "y_pred_mlp = (mlp_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"MLP Accuracy: {acc_mlp:.4f}\")\n",
    "\n",
    "# --- SHAP Analysis for All Models ---\n",
    "\n",
    "# Function to plot SHAP values\n",
    "def plot_shap_summary(shap_values, X_sample, model_name):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"{model_name} SHAP Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# SHAP for Logistic Regression\n",
    "explainer_lr = shap.Explainer(lr, X_train)\n",
    "shap_values_lr = explainer_lr(X_test)\n",
    "plot_shap_summary(shap_values_lr, X_test, \"Logistic Regression\")\n",
    "\n",
    "# SHAP for Random Forest\n",
    "explainer_rf = shap.Explainer(rf)\n",
    "shap_values_rf = explainer_rf(X_test)\n",
    "plot_shap_summary(shap_values_rf, X_test, \"Random Forest\")\n",
    "\n",
    "# SHAP for XGBoost\n",
    "explainer_xgb = shap.Explainer(xgb)\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "plot_shap_summary(shap_values_xgb, X_test, \"XGBoost\")\n",
    "\n",
    "# SHAP for MLP (Deep Learning Model)\n",
    "explainer_mlp = shap.DeepExplainer(mlp_model, X_train[:50])  # Use a small batch to approximate SHAP values\n",
    "shap_values_mlp = explainer_mlp.shap_values(X_test[:50])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "shap.summary_plot(shap_values_mlp[0], X_test[:50], plot_type=\"bar\", show=False)\n",
    "plt.title(\"MLP SHAP Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SHAP Analysis for All Models ---\n",
    "feature_names = X_test.columns  # Store feature names\n",
    "\n",
    "# Function to plot SHAP values safely\n",
    "def plot_shap_summary(shap_values, X_sample, model_name):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"{model_name} SHAP Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# SHAP for Logistic Regression\n",
    "explainer_lr = shap.Explainer(lr, X_train)\n",
    "shap_values_lr = explainer_lr(X_test.to_numpy())  # Convert DataFrame to NumPy\n",
    "plot_shap_summary(shap_values_lr, X_test.to_numpy(), \"Logistic Regression\")\n",
    "\n",
    "# SHAP for Random Forest (Fixed)\n",
    "explainer_rf = shap.Explainer(rf, X_train)\n",
    "shap_values_rf = explainer_rf(X_test.to_numpy())  # Convert DataFrame to NumPy\n",
    "plot_shap_summary(shap_values_rf, X_test.to_numpy(), \"Random Forest\")\n",
    "\n",
    "# SHAP for XGBoost\n",
    "explainer_xgb = shap.Explainer(xgb)\n",
    "shap_values_xgb = explainer_xgb(X_test.to_numpy())\n",
    "plot_shap_summary(shap_values_xgb, X_test.to_numpy(), \"XGBoost\")\n",
    "\n",
    "# SHAP for MLP (Deep Learning Model)\n",
    "explainer_mlp = shap.DeepExplainer(mlp_model, X_train[:50].to_numpy())  # Use a small batch\n",
    "shap_values_mlp = explainer_mlp.shap_values(X_test[:50].to_numpy())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "shap.summary_plot(shap_values_mlp[0], X_test[:50].to_numpy(), feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "plt.title(\"MLP SHAP Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize SHAP explainers for all models\n",
    "explainer_lr = shap.Explainer(lr, X_train)\n",
    "explainer_rf = shap.KernelExplainer(rf.predict, X_train[:100])  # Use a subset for efficiency\n",
    "explainer_xgb = shap.Explainer(xgb)\n",
    "explainer_mlp = shap.DeepExplainer(mlp_model, X_train[:50].to_numpy())  # MLP requires a small training batch\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values_lr = explainer_lr(X_test.to_numpy())\n",
    "shap_values_rf = explainer_rf.shap_values(X_test[:50].to_numpy())  # Random Forest (subset)\n",
    "shap_values_xgb = explainer_xgb(X_test.to_numpy())\n",
    "shap_values_mlp = explainer_mlp.shap_values(X_test[:50].to_numpy())  # MLP\n",
    "\n",
    "# Function to plot SHAP values safely\n",
    "def plot_shap_summary(shap_values, X_sample, model_name):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"{model_name} SHAP Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot SHAP values for all models\n",
    "plot_shap_summary(shap_values_lr, X_test.to_numpy(), \"Logistic Regression\")\n",
    "plot_shap_summary(shap_values_rf, X_test[:50].to_numpy(), \"Random Forest\")\n",
    "plot_shap_summary(shap_values_xgb, X_test.to_numpy(), \"XGBoost\")\n",
    "plot_shap_summary(shap_values_mlp, X_test[:50].to_numpy(), \"MLP Neural Network\")  # No [0] index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert X_test back to a NumPy array (SHAP for MLP requires NumPy)\n",
    "X_test_np = np.array(X_test)\n",
    "\n",
    "# Use SHAP's DeepExplainer for neural networks\n",
    "explainer_mlp = shap.Explainer(mlp_model, X_train)  # Use training data to initialize\n",
    "shap_values_mlp = explainer_mlp(X_test_np)\n",
    "\n",
    "# Plot summary of SHAP values\n",
    "plt.figure(figsize=(6, 4))\n",
    "shap.summary_plot(shap_values_mlp, X_test_np, feature_names=feature_cols, show=False)\n",
    "plt.title(\"MLP SHAP Feature Importance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
