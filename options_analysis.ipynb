{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to directories\n",
    "daily_volume_dir = \"/content/drive/MyDrive/FYP/cboe_options/cboe_options_filtered\"\n",
    "adv_dir = \"/content/drive/MyDrive/FYP/cboe_options/cboe_adv_filtered\"\n",
    "output_dir = \"/content/drive/MyDrive/FYP/cboe_options/cboe_options_with_adv\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each year\n",
    "for daily_file in os.listdir(daily_volume_dir):\n",
    "    if daily_file.startswith(\"filtered_daily_volume_\") and daily_file.endswith(\".csv\"):\n",
    "        year = daily_file.split(\"_\")[-1].replace(\".csv\", \"\")  # Extract year\n",
    "        print(f\"Processing year: {year}\")\n",
    "\n",
    "        # Load the daily volume and ADV files\n",
    "        daily_path = os.path.join(daily_volume_dir, daily_file)\n",
    "        daily_df = pd.read_csv(daily_path)\n",
    "\n",
    "        adv_file = f\"filtered_monthly_volume_{year}.csv\"\n",
    "        adv_path = os.path.join(adv_dir, adv_file)\n",
    "        adv_df = pd.read_csv(adv_path)\n",
    "\n",
    "        # Extract year and month from 'Trade Date' in daily data\n",
    "        daily_df['Trade Month'] = daily_df['Trade Date'].str[:7]  # Keep only year and month (e.g., \"2024/01\")\n",
    "\n",
    "        # Merge ADV data with daily volume data\n",
    "        merged_df = pd.merge(\n",
    "            daily_df,\n",
    "            adv_df.rename(columns={\"Trade Month\": \"Trade Month\"}),  # Ensure columns match\n",
    "            how=\"left\",\n",
    "            on=[\"Trade Month\", \"Options Class\"]\n",
    "        )\n",
    "\n",
    "        # Save the output for the year\n",
    "        output_file = os.path.join(output_dir, f\"daily_volume_with_adv_{year}.csv\")\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved combined data for {year} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory_path = \"/content/drive/MyDrive/FYP/cboe_options/cboe_options_with_adv\"\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = \"/content/drive/MyDrive/FYP/merged_cboe_options.csv\"\n",
    "\n",
    "# List to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Drop the specified columns if they exist\n",
    "        columns_to_drop = ['Trade Month', 'Underlying_y', 'Product Type_y', 'Exchange_y']\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"Merged CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV file\n",
    "merged_file_path = \"/content/drive/MyDrive/FYP/merged_cboe_options.csv\"\n",
    "output_file_path = \"/content/drive/MyDrive/FYP/cleaned_merged_cboe_options.csv\"\n",
    "\n",
    "# Read the merged file\n",
    "df = pd.read_csv(merged_file_path)\n",
    "\n",
    "# 1. Rename columns to remove the `_x` suffix\n",
    "df.rename(columns={\n",
    "    'Underlying_x': 'Underlying',\n",
    "    'Product Type_x': 'Product Type',\n",
    "    'Exchange_x': 'Exchange'\n",
    "}, inplace=True)\n",
    "\n",
    "# 2. Drop rows where `Underlying` is not equal to `Options Class`\n",
    "df = df[df['Underlying'] == df['Options Class']]\n",
    "\n",
    "# 3. Add a `% Spike` column to calculate the percentage difference\n",
    "# Formula: % Spike = ((Volume - Average Daily Volume) / Average Daily Volume) * 100\n",
    "df['% Spike'] = ((df['Volume'] - df['Average Daily Volume']) / df['Average Daily Volume']) * 100\n",
    "\n",
    "# Save the cleaned file\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned and updated data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "cboe_file_path = \"/content/drive/MyDrive/FYP/cleaned_merged_cboe_options.csv\"\n",
    "articles_file_path = \"/content/drive/MyDrive/FYP/articles_with_reddit_sentiment_all.csv\"\n",
    "output_file_path = \"/content/drive/MyDrive/FYP/merged_articles_with_reddit_with_options_data.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "cboe_df = pd.read_csv(cboe_file_path)\n",
    "articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "\n",
    "# 1. Rename columns in the CBOE dataset\n",
    "cboe_df.rename(columns={\n",
    "    'Volume': 'Options Volume',\n",
    "    'Average Daily Volume': 'Options Average Daily Volume',\n",
    "    '% Spike': 'Options % Spike'\n",
    "}, inplace=True)\n",
    "\n",
    "# 2. Convert date formats to the same standard\n",
    "cboe_df['Trade Date'] = pd.to_datetime(cboe_df['Trade Date'], format='%Y/%m/%d')\n",
    "articles_df['Date'] = pd.to_datetime(articles_df['Date'], format='%Y-%m-%d')\n",
    "print(\"trade date structure\", cboe_df['Trade Date'])\n",
    "print(\"article date structure\", articles_df['Date'])\n",
    "\n",
    "# 3. Perform the merge on 'Trade Date' -> 'Date' and 'Underlying' -> 'ticker'\n",
    "merged_df = pd.merge(\n",
    "    articles_df,\n",
    "    cboe_df,\n",
    "    left_on=['Date', 'ticker'],\n",
    "    right_on=['Trade Date', 'Underlying'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Drop unnecessary columns added during the merge\n",
    "merged_df.drop(columns=['Trade Date', 'Underlying', 'Options Class', 'Product Type', 'Exchange'], inplace=True)\n",
    "\n",
    "\n",
    "# 5. Save the final merged dataset\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing dates where articles were published to dates where they weren't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File paths\n",
    "csv1_path = \"/content/drive/MyDrive/FYP/merged_articles_with_reddit_with_options_data.csv\"\n",
    "csv2_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "csv1_df = pd.read_csv(csv1_path)\n",
    "csv2_df = pd.read_csv(csv2_path)\n",
    "\n",
    "# Ensure the 'Options % Spike' column is numeric\n",
    "csv1_df['Options % Spike'] = pd.to_numeric(csv1_df['Options % Spike'].abs(), errors='coerce')\n",
    "csv2_df['Options % Spike'] = pd.to_numeric(csv2_df['Options % Spike'].abs(), errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the 'Options % Spike' column\n",
    "csv1_df = csv1_df.dropna(subset=['Options % Spike'])\n",
    "csv2_df = csv2_df.dropna(subset=['Options % Spike'])\n",
    "\n",
    "# Add a column to indicate the source\n",
    "csv1_df['Source'] = 'Normal Dates'\n",
    "csv2_df['Source'] = 'Missing Dates'\n",
    "\n",
    "# Combine the two datasets and reset the index\n",
    "combined_df = pd.concat([csv1_df[['Options % Spike', 'Source']], csv2_df[['Options % Spike', 'Source']]]).reset_index(drop=True)\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(data=combined_df, x='Options % Spike', hue='Source', fill=True, common_norm=False, alpha=0.5)\n",
    "plt.title(\"Comparison of Options % Spike for Normal vs Missing Dates\")\n",
    "plt.xlabel(\"Options % Spike\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=combined_df, x='Source', y='Options % Spike')\n",
    "plt.title(\"Boxplot of Options % Spike for Normal vs Missing Dates\")\n",
    "plt.xlabel(\"Source\")\n",
    "plt.ylabel(\"Options % Spike\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the two datasets and reset the index\n",
    "combined_df = pd.concat([csv1_df[['Options % Spike', 'Source']], csv2_df[['Options % Spike', 'Source']]]).reset_index(drop=True)\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = combined_df.groupby('Source')['Options % Spike'].describe()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics for Options % Spike:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Exclude rows where 'Options % Spike' is infinite\n",
    "combined_df = combined_df[~combined_df['Options % Spike'].isin([float('inf'), float('-inf')])]\n",
    "\n",
    "# Recalculate summary statistics\n",
    "filtered_summary_stats = combined_df.groupby('Source')['Options % Spike'].describe()\n",
    "\n",
    "# Display filtered summary statistics\n",
    "print(\"Filtered Summary Statistics for Options % Spike:\")\n",
    "print(filtered_summary_stats)\n",
    "\n",
    "# Compare means\n",
    "mean_missing = filtered_summary_stats.loc['Missing Dates', 'mean']\n",
    "mean_normal = filtered_summary_stats.loc['Normal Dates', 'mean']\n",
    "\n",
    "if mean_missing > mean_normal:\n",
    "    print(f\"The average spike is greater for missing dates ({mean_missing:.2f}) than for normal dates ({mean_normal:.2f}).\")\n",
    "else:\n",
    "    print(f\"The average spike is greater for normal dates ({mean_normal:.2f}) than for missing dates ({mean_missing:.2f}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path for the dataset\n",
    "articles_file_path = \"/content/drive/MyDrive/FYP/merged_articles_with_reddit_with_options_data.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "# Ensure relevant columns are in the correct numeric format\n",
    "articles_df['% Spike'] = pd.to_numeric(articles_df['% Spike'].abs(), errors='coerce')\n",
    "articles_df['reddit vader sentiment'] = pd.to_numeric(articles_df['reddit vader sentiment'].abs(), errors='coerce')\n",
    "articles_df['Options % Spike'] = pd.to_numeric(articles_df['Options % Spike'].abs(), errors='coerce')\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'].abs(), errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the relevant columns\n",
    "correlation_df = articles_df[['Sentiment_Score', '% Spike', 'reddit vader sentiment', 'Options % Spike']].dropna()\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = correlation_df.corr()\n",
    "\n",
    "# Display the correlations between Sentiment_Score and the other columns\n",
    "print(\"Correlation of Sentiment_Score with other metrics:\")\n",
    "print(correlations['Sentiment_Score'][['% Spike', 'reddit vader sentiment', 'Options % Spike']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path for the dataset\n",
    "articles_file_path = \"/content/drive/MyDrive/FYP/merged_articles_with_reddit_with_options_data.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "# Drop all rows with any NaN values\n",
    "articles_df_cleaned = articles_df.dropna()\n",
    "\n",
    "# Display the number of rows before and after cleaning\n",
    "print(f\"Number of rows before cleaning: {len(articles_df)}\")\n",
    "print(f\"Number of rows after cleaning: {len(articles_df_cleaned)}\")\n",
    "\n",
    "# Save the cleaned DataFrame if needed\n",
    "output_file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"\n",
    "articles_df_cleaned.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File path for the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "articles_df = pd.read_csv(file_path)\n",
    "\n",
    "# Feature (Sentiment_Score) and target (Options % Spike)\n",
    "X = articles_df[['Sentiment_Score']]\n",
    "y = articles_df['Options % Spike']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.6, label='Predicted')\n",
    "plt.title('Linear Regression: Predicting Options % Spike')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Options % Spike')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Save the model if needed\n",
    "import joblib\n",
    "model_file_path = \"/content/drive/MyDrive/FYP/options_spike_prediction_model.pkl\"\n",
    "joblib.dump(model, model_file_path)\n",
    "print(f\"Model saved to: {model_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Use absolute values for the target variable\n",
    "data['Options % Spike'] = data['Options % Spike'].abs()\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Sentiment_Score']]  # Predictor: Sentiment Score\n",
    "y = data['Options % Spike']    # Target: Absolute Options % Spike\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2, label=\"Ideal Fit\")\n",
    "plt.title(\"Linear Regression: Actual vs Predicted Options % Spike\")\n",
    "plt.xlabel(\"Actual Absolute Options % Spike\")\n",
    "plt.ylabel(\"Predicted Absolute Options % Spike\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Display the regression coefficient and intercept\n",
    "print(f\"Regression Coefficient (Slope): {model.coef_[0]}\")\n",
    "print(f\"Regression Intercept: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/FYP/cleaned_final.csv\")\n",
    "\n",
    "# Preprocessing: Drop rows with missing values and use absolute values\n",
    "data = data.dropna()\n",
    "data['Options % Spike'] = data['Options % Spike'].abs()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data[['Sentiment_Score']]\n",
    "y = data['Options % Spike']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Visualize the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)\n",
    "plt.xlabel(\"Actual Options % Spike\")\n",
    "plt.ylabel(\"Predicted Options % Spike\")\n",
    "plt.title(\"Actual vs Predicted Options % Spike (Random Forest)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Initialize Neural Network Regressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_mlp = mean_squared_error(y_test, y_pred_mlp)\n",
    "r2_mlp = r2_score(y_test, y_pred_mlp)\n",
    "\n",
    "print(f\"Neural Network Regressor - Mean Squared Error: {mse_mlp}\")\n",
    "print(f\"Neural Network Regressor - R-squared: {r2_mlp}\")\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_mlp, alpha=0.5, edgecolors=\"k\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title(\"Actual vs Predicted Options % Spike (Neural Network Regressor)\")\n",
    "plt.xlabel(\"Actual Options % Spike\")\n",
    "plt.ylabel(\"Predicted Options % Spike\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Title', 'Options % Spike'])\n",
    "\n",
    "# Preprocessing: Target variable and feature\n",
    "y = df['Options % Spike']  # Use absolute values of % Spike\n",
    "X = df['Title']\n",
    "\n",
    "# Convert text to numerical data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, edgecolors=\"k\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title(\"Actual vs Predicted Options % Spike (Gradient Boosting Regressor)\")\n",
    "plt.xlabel(\"Actual Options % Spike\")\n",
    "plt.ylabel(\"Predicted Options % Spike\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Title', 'Options % Spike'])\n",
    "\n",
    "# Use absolute values of `Options % Spike` as target\n",
    "df['Options % Spike'] = df['Options % Spike'].abs()\n",
    "\n",
    "# Text data: Title (input) and Options % Spike (target)\n",
    "texts = df['Title'].values\n",
    "target = df['Options % Spike'].values\n",
    "\n",
    "# Text Preprocessing\n",
    "max_words = 10000  # Maximum number of words in the vocabulary\n",
    "max_len = 100  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = target\n",
    "\n",
    "# Scale the target variable\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_scaled))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaler.inverse_transform(y_test), y_pred, alpha=0.5, edgecolors=\"k\")\n",
    "plt.plot(\n",
    "    [scaler.inverse_transform(y_test).min(), scaler.inverse_transform(y_test).max()],\n",
    "    [scaler.inverse_transform(y_test).min(), scaler.inverse_transform(y_test).max()],\n",
    "    'r--',\n",
    "    lw=2\n",
    ")\n",
    "plt.title(\"Actual vs Predicted Options % Spike (LSTM)\")\n",
    "plt.xlabel(\"Actual Options % Spike\")\n",
    "plt.ylabel(\"Predicted Options % Spike\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"  # Update this to your file's path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing values to ensure valid correlation calculations\n",
    "data = data.dropna(subset=['Sentiment_Score', 'Options % Spike'])\n",
    "\n",
    "# Group by company and calculate correlation\n",
    "correlations = data.groupby('Company').apply(\n",
    "    lambda group: group['Sentiment_Score'].corr(group['Options % Spike'])\n",
    ")\n",
    "\n",
    "# Find the company with the highest correlation\n",
    "highest_correlation = correlations.idxmax()\n",
    "max_correlation_value = correlations.max()\n",
    "\n",
    "print(f\"The company with the greatest correlation is: {highest_correlation}\")\n",
    "print(f\"Correlation value: {max_correlation_value}\")\n",
    "\n",
    "# Display all correlations for context\n",
    "print(\"\\nAll companies with their correlation values:\")\n",
    "print(correlations.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/cleaned_final.csv\"  # Update with your dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure numerical columns are in the correct format\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "df['% Spike'] = pd.to_numeric(df['% Spike'], errors='coerce')\n",
    "df['reddit vader sentiment'] = pd.to_numeric(df['reddit vader sentiment'], errors='coerce')\n",
    "df['Options % Spike'] = pd.to_numeric(df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['Sentiment_Score', '% Spike', 'reddit vader sentiment', 'Options % Spike'], inplace=True)\n",
    "\n",
    "# Group by company\n",
    "company_groups = df.groupby('Company')\n",
    "\n",
    "# Calculate correlations for each company\n",
    "correlation_results = []\n",
    "for company, group in company_groups:\n",
    "    sentiment_spike_corr = group['Sentiment_Score'].corr(group['% Spike'])\n",
    "    sentiment_reddit_corr = group['Sentiment_Score'].corr(group['reddit vader sentiment'])\n",
    "    sentiment_options_spike_corr = group['Sentiment_Score'].corr(group['Options % Spike'])\n",
    "\n",
    "    correlation_results.append({\n",
    "        'Company': company,\n",
    "        'Sentiment_Score vs % Spike': sentiment_spike_corr,\n",
    "        'Sentiment_Score vs reddit vader sentiment': sentiment_reddit_corr,\n",
    "        'Sentiment_Score vs Options % Spike': sentiment_options_spike_corr\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the correlation results\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# Find top performers for each correlation\n",
    "top_sentiment_spike = correlation_df.sort_values(by='Sentiment_Score vs % Spike', ascending=False).head(5)\n",
    "top_sentiment_reddit = correlation_df.sort_values(by='Sentiment_Score vs reddit vader sentiment', ascending=False).head(5)\n",
    "top_sentiment_options_spike = correlation_df.sort_values(by='Sentiment_Score vs Options % Spike', ascending=False).head(5)\n",
    "\n",
    "# Save the correlation results\n",
    "correlation_df.to_csv(\"company_correlations.csv\", index=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top Companies - Sentiment_Score vs % Spike:\")\n",
    "print(top_sentiment_spike)\n",
    "print(\"\\nTop Companies - Sentiment_Score vs reddit vader sentiment:\")\n",
    "print(top_sentiment_reddit)\n",
    "print(\"\\nTop Companies - Sentiment_Score vs Options % Spike:\")\n",
    "print(top_sentiment_options_spike)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure numerical columns are in the correct format\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "df['% Spike'] = pd.to_numeric(df['% Spike'], errors='coerce')\n",
    "df['reddit vader sentiment'] = pd.to_numeric(df['reddit vader sentiment'], errors='coerce')\n",
    "df['Options % Spike'] = pd.to_numeric(df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['Sentiment_Score', '% Spike', 'reddit vader sentiment', 'Options % Spike'], inplace=True)\n",
    "\n",
    "# Group by company\n",
    "company_groups = df.groupby('Company')\n",
    "\n",
    "# Calculate correlations for each company\n",
    "correlation_results = []\n",
    "for company, group in company_groups:\n",
    "    sentiment_spike_corr = group['Sentiment_Score'].corr(group['% Spike'])\n",
    "    sentiment_reddit_corr = group['Sentiment_Score'].corr(group['reddit vader sentiment'])\n",
    "    sentiment_options_spike_corr = group['Sentiment_Score'].corr(group['Options % Spike'])\n",
    "\n",
    "    # Aggregate score as the sum of absolute correlations\n",
    "    aggregate_score = (\n",
    "        abs(sentiment_spike_corr) +\n",
    "        abs(sentiment_reddit_corr) +\n",
    "        abs(sentiment_options_spike_corr)\n",
    "    )\n",
    "\n",
    "    correlation_results.append({\n",
    "        'Company': company,\n",
    "        'Sentiment_Score vs % Spike': sentiment_spike_corr,\n",
    "        'Sentiment_Score vs reddit vader sentiment': sentiment_reddit_corr,\n",
    "        'Sentiment_Score vs Options % Spike': sentiment_options_spike_corr,\n",
    "        'Aggregate Score': aggregate_score\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the correlation results\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# Find top companies based on aggregate score\n",
    "top_companies = correlation_df.sort_values(by='Aggregate Score', ascending=False).head(10)\n",
    "\n",
    "# Save the correlation results\n",
    "correlation_df.to_csv(\"company_correlations_with_aggregate_score.csv\", index=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top Companies by Aggregate Correlation Score:\")\n",
    "print(top_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure numerical columns are in the correct format\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "df['% Spike'] = pd.to_numeric(df['% Spike'], errors='coerce')\n",
    "df['reddit vader sentiment'] = pd.to_numeric(df['reddit vader sentiment'], errors='coerce')\n",
    "df['Options % Spike'] = pd.to_numeric(df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['Sentiment_Score', '% Spike', 'reddit vader sentiment', 'Options % Spike'], inplace=True)\n",
    "\n",
    "# Group by company\n",
    "company_groups = df.groupby('Company')\n",
    "\n",
    "# Calculate correlations for each company\n",
    "correlation_results = []\n",
    "for company, group in company_groups:\n",
    "    sentiment_spike_corr = group['Sentiment_Score'].corr(group['% Spike'])\n",
    "    sentiment_reddit_corr = group['Sentiment_Score'].corr(group['reddit vader sentiment'])\n",
    "    sentiment_options_spike_corr = group['Sentiment_Score'].corr(group['Options % Spike'])\n",
    "\n",
    "    # Aggregate score as the sum of absolute correlations\n",
    "    aggregate_score = (\n",
    "        abs(sentiment_spike_corr) +\n",
    "        abs(sentiment_reddit_corr) +\n",
    "        abs(sentiment_options_spike_corr)\n",
    "    )\n",
    "\n",
    "    correlation_results.append({\n",
    "        'Company': company,\n",
    "        'Sentiment_Score vs % Spike': sentiment_spike_corr,\n",
    "        'Sentiment_Score vs reddit vader sentiment': sentiment_reddit_corr,\n",
    "        'Sentiment_Score vs Options % Spike': sentiment_options_spike_corr,\n",
    "        'Aggregate Score': aggregate_score\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the correlation results\n",
    "correlation_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# Find top companies based on aggregate score\n",
    "top_companies = correlation_df.sort_values(by='Aggregate Score', ascending=False).head(10)\n",
    "\n",
    "# Merge the top companies back with the original dataset to analyze common factors\n",
    "top_company_names = top_companies['Company']\n",
    "top_company_data = df[df['Company'].isin(top_company_names)]\n",
    "\n",
    "# Analyze common factors\n",
    "common_factors = top_company_data.groupby('Company').agg({\n",
    "    'Sentiment_Score': ['mean', 'std'],\n",
    "    '% Spike': ['mean', 'std'],\n",
    "    'reddit vader sentiment': ['mean', 'std'],\n",
    "    'Options % Spike': ['mean', 'std'],\n",
    "    'Daily Trading Volume': 'mean',  # Assuming this column exists\n",
    "    'Monthly Average Volume': 'mean',  # Assuming this column exists\n",
    "})\n",
    "\n",
    "# Display common factors\n",
    "print(\"Common Factors for Top Companies:\")\n",
    "print(common_factors)\n",
    "\n",
    "# Save common factors to a CSV file for detailed analysis\n",
    "common_factors.to_csv(\"common_factors_for_top_companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Add a Sector column manually or map sectors using a predefined dictionary\n",
    "sector_mapping = {\n",
    "    \"AMERICAN TOWER CORP\": \"Real Estate\",\n",
    "    \"AMGEN Inc\": \"Healthcare\",\n",
    "    \"AUTOMATIC DATA PROCESSING INC\": \"Technology\",\n",
    "    \"CHARTER COMMUNICATIONS INC\": \"Communication Services\",\n",
    "    \"CHUBB LTD\": \"Financials\",\n",
    "    \"MEDTRONIC PLC\": \"Healthcare\",\n",
    "    \"MERCK & CO INC\": \"Healthcare\",\n",
    "    \"NEWMONT CORP\": \"Materials\",\n",
    "    \"NORTHROP GRUMMAN CORP\": \"Industrials\",\n",
    "    \"VERTEX PHARMACEUTICALS INC\": \"Healthcare\",\n",
    "}\n",
    "data[\"Sector\"] = data[\"Company\"].map(sector_mapping)\n",
    "\n",
    "# Group by sector and calculate aggregate metrics\n",
    "sector_summary = data.groupby(\"Sector\").agg(\n",
    "    {\n",
    "        \"Sentiment_Score\": [\"mean\", \"std\"],\n",
    "        \"% Spike\": [\"mean\", \"std\"],\n",
    "        \"reddit vader sentiment\": [\"mean\", \"std\"],\n",
    "        \"Options % Spike\": [\"mean\", \"std\"],\n",
    "        \"Daily Trading Volume\": \"mean\",\n",
    "        \"Monthly Average Volume\": \"mean\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Sector-Wise Summary:\")\n",
    "print(sector_summary)\n",
    "\n",
    "# Visualize sector-wise options spike\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sector_options_spike = data.groupby(\"Sector\")[\"Options % Spike\"].mean()\n",
    "sector_options_spike.plot(kind=\"bar\", figsize=(10, 6), title=\"Sector-Wise Options % Spike\")\n",
    "plt.xlabel(\"Sector\")\n",
    "plt.ylabel(\"Options % Spike\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
