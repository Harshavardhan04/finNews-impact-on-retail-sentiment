{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets\n",
    "!pip install pandas\n",
    "!pip install nltk emoji contractions vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  analysis performed using dataset found online linked below (credit to Theriley106) ",
    " integrated with reddit comments processed in reddit_sentiment.ipynb\n",
    "\n",
    "import opendatasets as od\n",
    "import pandas\n",
    "\n",
    "od.download(\n",
    "\t\"https://www.kaggle.com/datasets/theriley106/wallstreetbetscomments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_file =('wallstreetbetscomments/\\\n",
    "wsbData.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: for lemmatization\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install kagglehub pandas transformers scikit-learn nltk spacy\n",
    "\n",
    "# Import necessary libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "# Download dataset using kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"theriley106/wallstreetbetscomments\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Paths\n",
    "\n",
    "output_file = \"/content/relevant_wsb_comments.json\"\n",
    "\n",
    "# Initialize resources\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Ticker list\n",
    "tickers = [\n",
    "    \"AAPL\", \"ABBV\", \"ABT\", \"ACN\", \"ADBE\", \"ADP\", \"AMAT\", \"AMD\", \"AMGN\", \"AMT\", \"AMZN\", \"APH\", \"AVGO\", \"AZO\", \"BA\",\n",
    "    \"BAC\", \"BDX\", \"BMY\", \"C\", \"CB\", \"CHTR\", \"CMCSA\", \"COP\", \"COST\", \"CSCO\", \"CVS\", \"CVX\", \"DELL\", \"DHR\", \"DIS\", \"DUK\",\n",
    "    \"ED\", \"GE\", \"GILD\", \"GIS\", \"GOOGL\", \"HD\", \"IBM\", \"INTC\", \"INTU\", \"ISRG\", \"JNJ\", \"JPM\", \"KHC\", \"KO\", \"KR\", \"LLY\",\n",
    "    \"LMT\", \"MA\", \"MDLZ\", \"MDT\", \"META\", \"MMM\", \"MO\", \"MRK\", \"MSFT\", \"MSI\", \"NEE\", \"NEM\", \"NFLX\", \"NKE\", \"NOC\", \"NVDA\",\n",
    "    \"ORCL\", \"PAYX\", \"PEP\", \"PFE\", \"PG\", \"PGR\", \"PM\", \"PSA\", \"PYPL\", \"QCOM\", \"TMO\", \"TMUS\", \"TSLA\", \"TXN\", \"UNH\", \"UPS\",\n",
    "    \"V\", \"VRTX\", \"VZ\", \"WCN\", \"WFC\", \"WMT\", \"XOM\", \"YUM\"\n",
    "]\n",
    "\n",
    "# Advanced cleaning function\n",
    "def advanced_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'(\\s+)', ' ', text)  # Remove extra spaces\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Check ticker relevance\n",
    "def is_ticker_mentioned(text, tickers):\n",
    "    return any(ticker.lower() in text.lower() for ticker in tickers)\n",
    "\n",
    "# Extract financial entities\n",
    "def extract_financial_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\"]]\n",
    "\n",
    "# Calculate relevance using TF-IDF\n",
    "def calculate_relevance(text, tickers, vectorizer, tfidf_matrix):\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    similarity_scores = cosine_similarity(text_tfidf, tfidf_matrix).flatten()\n",
    "    return max(similarity_scores)\n",
    "\n",
    "# TF-IDF setup\n",
    "ticker_context = ' '.join(tickers)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([ticker_context])\n",
    "\n",
    "# Process JSON in chunks\n",
    "chunk_size = 10_000  # Adjust based on memory\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for chunk in pd.read_json(infile, lines=True, chunksize=chunk_size):  # Read JSON lines\n",
    "        relevant_data = []\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                text = row.get('body', '')  # Replace 'body' with the correct key in your JSON\n",
    "                if not isinstance(text, str) or not text.strip():\n",
    "                    continue\n",
    "\n",
    "                cleaned_text = advanced_cleaning(text)\n",
    "                if is_ticker_mentioned(cleaned_text, tickers):\n",
    "                    financial_entities = extract_financial_entities(cleaned_text)\n",
    "                    relevance_score = calculate_relevance(cleaned_text, tickers, vectorizer, tfidf_matrix)\n",
    "\n",
    "                    if relevance_score > 0.0:  # Adjust threshold as needed\n",
    "                        relevant_data.append({\n",
    "                            'Original_Text': text,\n",
    "                            'Cleaned_Text': cleaned_text,\n",
    "                            'Financial_Entities': financial_entities,\n",
    "                            'Relevance_Score': relevance_score,\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "\n",
    "        # Write relevant data to output file as JSON lines\n",
    "        for record in relevant_data:\n",
    "            outfile.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Relevant comments saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 1: Enhanced Text Cleaning\n",
    "# ---------------------------- #\n",
    "\n",
    "# Initialize NLTK resources\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis and non-ASCII characters.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions like don't -> do not.\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def advanced_cleaning(text):\n",
    "    \"\"\"Comprehensive text cleaning.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove emojis and non-ASCII characters\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 2: VADER Customization\n",
    "# ---------------------------- #\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Add custom WallStreetBets words to the VADER lexicon\n",
    "wsb_words = {\n",
    "    'citron': -4.0,\n",
    "    'hidenburg': -4.0,\n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,\n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,\n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "    'tendies': 2.0,\n",
    "    'town': 2.0,\n",
    "    'overvalued': -3.0,\n",
    "    'undervalued': 3.0,\n",
    "    'buy': 4.0,\n",
    "    'sell': -4.0,\n",
    "    'gone': -1.0,\n",
    "    'gtfo': -1.7,\n",
    "    'paper': -1.7,\n",
    "    'bullish': 3.7,\n",
    "    'bearish': -3.7,\n",
    "    'bagholder': -1.7,\n",
    "    'stonk': 1.9,\n",
    "    'green': 1.9,\n",
    "    'money': 1.2,\n",
    "    'print': 2.2,\n",
    "    'rocket': 2.2,\n",
    "    'bull': 2.9,\n",
    "    'bear': -2.9,\n",
    "    'pumping': -1.0,\n",
    "    'sus': -3.0,\n",
    "    'offering': -2.3,\n",
    "    'rip': -4.0,\n",
    "    'downgrade': -3.0,\n",
    "    'upgrade': 3.0,\n",
    "    'maintain': 1.0,\n",
    "    'pump': 1.9,\n",
    "    'hot': 1.5,\n",
    "    'drop': -2.5,\n",
    "    'rebound': 1.5,\n",
    "    'crack': 2.5,\n",
    "}\n",
    "vader.lexicon.update(wsb_words)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 3: Apply Cleaning + Sentiment\n",
    "# ---------------------------- #\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = \"/content/drive/MyDrive/FYP/reddit part/json_ticker_csvs\"\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/FYP/reddit part/json_ticker_sentiment_csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        # Load the CSV file\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Ensure the 'body' column exists\n",
    "        if 'body' not in df.columns:\n",
    "            print(f\"Skipping {filename}: 'body' column not found.\")\n",
    "            continue\n",
    "\n",
    "        # Step 3.1: Apply enhanced text cleaning\n",
    "        df['cleaned_body'] = df['body'].apply(advanced_cleaning)\n",
    "\n",
    "        # Step 3.2: Apply sentiment analysis\n",
    "        df['sentiment_score'] = df['cleaned_body'].apply(lambda x: vader.polarity_scores(str(x))['compound'])\n",
    "\n",
    "        # Step 3.3: Label sentiment as Positive, Neutral, Negative\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(\n",
    "            lambda score: 'Positive' if score > 0.05 else ('Negative' if score < -0.05 else 'Neutral')\n",
    "        )\n",
    "\n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        output_filepath = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Finished processing {filename}. Saved to {output_filepath}.\")\n",
    "\n",
    "print(f\"Sentiment analysis completed. Results saved in '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/json_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows where Reddit_Vader_Sentiment or Sentiment_Score is blank or NaN\n",
    "filtered_df = df.dropna(subset=['Reddit_Vader_Sentiment', 'Sentiment_Score'])\n",
    "\n",
    "# Ensure both columns are numeric\n",
    "filtered_df['Reddit_Vader_Sentiment'] = pd.to_numeric(filtered_df['Reddit_Vader_Sentiment'], errors='coerce')\n",
    "filtered_df['Sentiment_Score'] = pd.to_numeric(filtered_df['Sentiment_Score'], errors='coerce')\n",
    "\n",
    "# Drop any remaining NaN values after conversion\n",
    "filtered_df = filtered_df.dropna(subset=['Reddit_Vader_Sentiment', 'Sentiment_Score'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation = filtered_df['Sentiment_Score'].corr(filtered_df['Reddit_Vader_Sentiment'])\n",
    "\n",
    "print(f\"Correlation between Sentiment_Score and Reddit_Vader_Sentiment: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Integrating in the 50GB dataset we preprocessed in reddit_sentiment.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 1: Enhanced Text Cleaning\n",
    "# ---------------------------- #\n",
    "\n",
    "# Initialize NLTK resources\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis and non-ASCII characters.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions like don't -> do not.\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def advanced_cleaning(text):\n",
    "    \"\"\"Comprehensive text cleaning.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove emojis and non-ASCII characters\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 2: VADER Customization\n",
    "# ---------------------------- #\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Add custom WallStreetBets words to the VADER lexicon\n",
    "wsb_words = {\n",
    "    'citron': -4.0,\n",
    "    'hidenburg': -4.0,\n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,\n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,\n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "    'tendies': 2.0,\n",
    "    'town': 2.0,\n",
    "    'overvalued': -3.0,\n",
    "    'undervalued': 3.0,\n",
    "    'buy': 4.0,\n",
    "    'sell': -4.0,\n",
    "    'gone': -1.0,\n",
    "    'gtfo': -1.7,\n",
    "    'paper': -1.7,\n",
    "    'bullish': 3.7,\n",
    "    'bearish': -3.7,\n",
    "    'bagholder': -1.7,\n",
    "    'stonk': 1.9,\n",
    "    'green': 1.9,\n",
    "    'money': 1.2,\n",
    "    'print': 2.2,\n",
    "    'rocket': 2.2,\n",
    "    'bull': 2.9,\n",
    "    'bear': -2.9,\n",
    "    'pumping': -1.0,\n",
    "    'sus': -3.0,\n",
    "    'offering': -2.3,\n",
    "    'rip': -4.0,\n",
    "    'downgrade': -3.0,\n",
    "    'upgrade': 3.0,\n",
    "    'maintain': 1.0,\n",
    "    'pump': 1.9,\n",
    "    'hot': 1.5,\n",
    "    'drop': -2.5,\n",
    "    'rebound': 1.5,\n",
    "    'crack': 2.5,\n",
    "}\n",
    "vader.lexicon.update(wsb_words)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Step 3: Apply Cleaning + Sentiment\n",
    "# ---------------------------- #\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = \"/content/drive/MyDrive/FYP/reddit part/ticker_csvs\"\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/FYP/reddit part/v2_ticker_sentiment_csvs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        # Load the CSV file\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Ensure the 'body' column exists\n",
    "        if 'body' not in df.columns:\n",
    "            print(f\"Skipping {filename}: 'body' column not found.\")\n",
    "            continue\n",
    "\n",
    "        # Step 3.1: Apply enhanced text cleaning\n",
    "        df['cleaned_body'] = df['body'].apply(advanced_cleaning)\n",
    "\n",
    "        # Step 3.2: Apply sentiment analysis\n",
    "        df['sentiment_score'] = df['cleaned_body'].apply(lambda x: vader.polarity_scores(str(x))['compound'])\n",
    "\n",
    "        # Step 3.3: Label sentiment as Positive, Neutral, Negative\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(\n",
    "            lambda score: 'Positive' if score > 0.05 else ('Negative' if score < -0.05 else 'Neutral')\n",
    "        )\n",
    "\n",
    "        # Save the updated dataframe to a new CSV file\n",
    "        output_filepath = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Finished processing {filename}. Saved to {output_filepath}.\")\n",
    "\n",
    "print(f\"Sentiment analysis completed. Results saved in '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File paths\n",
    "articles_file = \"/content/drive/MyDrive/FYP/merged_articles_with_reddit_with_options_data.csv\"\n",
    "reddit_sentiment_dir = \"/content/drive/MyDrive/FYP/reddit part/v2_ticker_sentiment_csvs\"\n",
    "output_file = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "\n",
    "# Load the articles dataset\n",
    "articles_df = pd.read_csv(articles_file)\n",
    "\n",
    "articles_df.drop(columns='reddit vader sentiment', inplace=True)\n",
    "\n",
    "\n",
    "# Ensure 'reddit vader sentiment' column exists\n",
    "if 'Reddit_Vader_Sentiment' not in articles_df.columns:\n",
    "    articles_df['Reddit_Vader_Sentiment'] = ''\n",
    "\n",
    "# Load existing progress if output file exists\n",
    "if os.path.exists(output_file):\n",
    "    updated_articles_df = pd.read_csv(output_file)\n",
    "    processed_tickers = set(updated_articles_df['ticker'].unique())\n",
    "else:\n",
    "    updated_articles_df = articles_df.copy()\n",
    "    processed_tickers = set()\n",
    "\n",
    "# List all Reddit sentiment files\n",
    "reddit_files = [f for f in os.listdir(reddit_sentiment_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Process each Reddit CSV\n",
    "for reddit_file in tqdm(reddit_files, desc=\"Processing Reddit Sentiment Files\"):\n",
    "    # Extract the ticker from the filename\n",
    "    ticker = os.path.splitext(reddit_file)[0]\n",
    "\n",
    "    # Skip already processed tickers\n",
    "    if ticker in processed_tickers:\n",
    "        print(f\"Skipping already processed ticker: {ticker}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing ticker: {ticker}...\")\n",
    "\n",
    "    # Load the Reddit sentiment data for the ticker\n",
    "    reddit_df = pd.read_csv(os.path.join(reddit_sentiment_dir, reddit_file))\n",
    "\n",
    "    # Handle invalid dates in the Reddit dataset\n",
    "    def safe_parse_date(date):\n",
    "        try:\n",
    "            return pd.to_datetime(date, format='mixed', dayfirst=True).strftime('%d/%m/%Y')\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Parse and filter valid dates\n",
    "    reddit_df['Formatted_Date'] = reddit_df['comment_date'].apply(safe_parse_date)\n",
    "    reddit_df = reddit_df[reddit_df['Formatted_Date'].notna()]\n",
    "\n",
    "    # Filter articles for the current ticker\n",
    "    ticker_articles_df = articles_df[articles_df['ticker'] == ticker]\n",
    "\n",
    "    # Convert articles dates to match the same format\n",
    "    ticker_articles_df['Formatted_Date'] = pd.to_datetime(\n",
    "        ticker_articles_df['Formatted_Date'], format='mixed', dayfirst=True\n",
    "    ).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    # Find common dates\n",
    "    common_dates = set(ticker_articles_df['Formatted_Date']).intersection(set(reddit_df['Formatted_Date']))\n",
    "\n",
    "    # Filter Reddit dataset to keep only rows with common dates\n",
    "    reddit_common_df = reddit_df[reddit_df['Formatted_Date'].isin(common_dates)]\n",
    "\n",
    "    # Calculate the average sentiment score for each common date\n",
    "    average_sentiments = reddit_common_df.groupby('Formatted_Date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "    # Map the average sentiment scores to the articles dataset\n",
    "    ticker_articles_df['Reddit_Vader_Sentiment'] = ticker_articles_df['Formatted_Date'].map(\n",
    "        average_sentiments.set_index('Formatted_Date')['sentiment_score']\n",
    "    )\n",
    "\n",
    "    # Update the main DataFrame\n",
    "    updated_articles_df.loc[updated_articles_df['ticker'] == ticker, 'Reddit_Vader_Sentiment'] = ticker_articles_df[\n",
    "        'Reddit_Vader_Sentiment'\n",
    "    ]\n",
    "\n",
    "    # Save progress after processing each ticker\n",
    "    updated_articles_df.to_csv(output_file, index=False)\n",
    "    print(f\"Progress saved for ticker: {ticker}\")\n",
    "\n",
    "print(f\"Sentiment analysis completed for all tickers. Final data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows where Reddit_Vader_Sentiment or Sentiment_Score is blank or NaN\n",
    "filtered_df = df.dropna(subset=['Reddit_Vader_Sentiment', 'Sentiment_Score'])\n",
    "\n",
    "# Ensure both columns are numeric\n",
    "filtered_df['Reddit_Vader_Sentiment'] = pd.to_numeric(filtered_df['Reddit_Vader_Sentiment'], errors='coerce')\n",
    "filtered_df['Sentiment_Score'] = pd.to_numeric(filtered_df['Sentiment_Score'], errors='coerce')\n",
    "\n",
    "# Drop any remaining NaN values after conversion\n",
    "filtered_df = filtered_df.dropna(subset=['Reddit_Vader_Sentiment', 'Sentiment_Score'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlation = filtered_df['Sentiment_Score'].corr(filtered_df['Reddit_Vader_Sentiment'])\n",
    "\n",
    "print(f\"Correlation between Sentiment_Score and Reddit_Vader_Sentiment: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"  # Update with the correct file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "#print(df.head())\n",
    "\n",
    "# Convert relevant columns to numeric, coercing errors to NaN for proper correlation calculation\n",
    "numeric_columns = [\n",
    "    'Sentiment_Score',\n",
    "    'Daily Trading Volume',\n",
    "    'Monthly Average Volume',\n",
    "    'Trading Volume',\n",
    "    '% Spike',\n",
    "    'Options Volume',\n",
    "    'Options Average Daily Volume',\n",
    "    'Options % Spike',\n",
    "    'Reddit_Vader_Sentiment'\n",
    "]\n",
    "\n",
    "# Convert columns to numeric and handle non-numeric values\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Compute the correlation matrix while handling NA values\n",
    "correlation_matrix = df[numeric_columns].corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optionally, save the correlation matrix to a CSV file\n",
    "correlation_matrix.to_csv(\"correlation_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"  # Update with the correct file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "\n",
    "\n",
    "# Columns related to trading volume\n",
    "volume_columns = [\n",
    "    'Daily Trading Volume',\n",
    "    'Monthly Average Volume',\n",
    "    'Trading Volume',\n",
    "    'Options Volume',\n",
    "    'Options Average Daily Volume'\n",
    "]\n",
    "\n",
    "# Relevant numerical columns for correlation\n",
    "numeric_columns = [\n",
    "    'Sentiment_Score',\n",
    "    '% Spike',\n",
    "    'Options % Spike',\n",
    "    'Reddit_Vader_Sentiment'\n",
    "] + volume_columns\n",
    "\n",
    "# Convert columns to numeric, handling non-numeric values\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Create new columns with log-transformed trading volume data (handling zero and negative values)\n",
    "for col in volume_columns:\n",
    "    df[f'Log_{col}'] = np.log1p(df[col])  # log1p handles log(0) safely\n",
    "\n",
    "# Add the log-transformed columns to the list for correlation\n",
    "log_columns = [f'Log_{col}' for col in volume_columns]\n",
    "all_numeric_columns = numeric_columns + log_columns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df[all_numeric_columns].corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix with Log-Transformed Volume Columns:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optionally, save the correlation matrix to a CSV file\n",
    "correlation_matrix.to_csv(\"correlation_matrix_with_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure proper datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing sentiment scores\n",
    "df.dropna(subset=['Sentiment_Score'], inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Temporal Sentiment Aggregation\n",
    "# ----------------------------\n",
    "\n",
    "def aggregate_sentiment(df, freq):\n",
    "    \"\"\"Aggregate sentiment scores by frequency (D=daily, W=weekly, M=monthly).\"\"\"\n",
    "    return df.groupby(['ticker', pd.Grouper(key='Date', freq=freq)])['Sentiment_Score'].mean().reset_index()\n",
    "\n",
    "daily_sentiment = aggregate_sentiment(df, 'D')\n",
    "weekly_sentiment = aggregate_sentiment(df, 'W')\n",
    "monthly_sentiment = aggregate_sentiment(df, 'M')\n",
    "\n",
    "# Plot sentiment trend for a specific company (e.g., AAPL)\n",
    "def plot_sentiment_trend(agg_df, ticker, freq):\n",
    "    company_data = agg_df[agg_df['ticker'] == ticker]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(company_data['Date'], company_data['Sentiment_Score'], marker='o')\n",
    "    plt.title(f'{ticker} {freq} Sentiment Trend')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sentiment')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_trend(daily_sentiment, 'AAPL', 'Daily')\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Topic Modeling with LDA\n",
    "# ----------------------------\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_Title'] = df['Title'].astype(str).apply(clean_text)\n",
    "\n",
    "# Convert text to Bag-of-Words\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(df['cleaned_Title'])\n",
    "\n",
    "# Apply LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[-no_top_words:]]))\n",
    "\n",
    "display_topics(lda_model, vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "# Assign dominant topic to each article\n",
    "topic_results = lda_model.transform(doc_term_matrix)\n",
    "df['dominant_topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Combining Sentiment and Topics\n",
    "# ----------------------------\n",
    "\n",
    "# Aggregate sentiment by topic\n",
    "topic_sentiment = df.groupby('dominant_topic')['Sentiment_Score'].mean().reset_index()\n",
    "print(\"\\nAverage Sentiment by Topic:\\n\", topic_sentiment)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Export Results\n",
    "# ----------------------------\n",
    "\n",
    "daily_sentiment.to_csv(\"/content/drive/MyDrive/FYP/daily_sentiment.csv\", index=False)\n",
    "weekly_sentiment.to_csv(\"/content/drive/MyDrive/FYP/weekly_sentiment.csv\", index=False)\n",
    "monthly_sentiment.to_csv(\"/content/drive/MyDrive/FYP/monthly_sentiment.csv\", index=False)\n",
    "df.to_csv(\"/content/drive/MyDrive/FYP/articles_with_topics.csv\", index=False)\n",
    "\n",
    "print(\"Aggregation and topic modeling completed. Files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the sentiment data\n",
    "daily_sentiment = pd.read_csv(\"/content/drive/MyDrive/FYP/daily_sentiment.csv\")\n",
    "weekly_sentiment = pd.read_csv(\"/content/drive/MyDrive/FYP/weekly_sentiment.csv\")\n",
    "monthly_sentiment = pd.read_csv(\"/content/drive/MyDrive/FYP/monthly_sentiment.csv\")\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "\n",
    "\n",
    "# Plotting function for sentiment trends\n",
    "def plot_sentiment_trend(agg_df, ticker, freq):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    company_data = agg_df[agg_df['ticker'] == ticker]\n",
    "    plt.plot(company_data['Date'], company_data['Sentiment_Score'], marker='o', label=f'{freq} Sentiment')\n",
    "    plt.title(f'{ticker} {freq} Sentiment Trend')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sentiment')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for a specific company (e.g., AAPL)\n",
    "ticker = 'AAPL'  # Replace with any ticker symbol you'd like to analyze\n",
    "\n",
    "# Plot daily, weekly, and monthly sentiment trends\n",
    "plot_sentiment_trend(daily_sentiment, ticker, 'Daily')\n",
    "plot_sentiment_trend(weekly_sentiment, ticker, 'Weekly')\n",
    "plot_sentiment_trend(monthly_sentiment, ticker, 'Monthly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Load the Dataset\n",
    "# -------------------------------------\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure proper datetime format and numeric types\n",
    "df['Formatted_Date'] = pd.to_datetime(df['Formatted_Date'], errors='coerce')\n",
    "df['Options Average Daily Volume'] = pd.to_numeric(df['Options Average Daily Volume'], errors='coerce')\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "df.dropna(subset=['Sentiment_Score', 'Options Average Daily Volume'], inplace=True)\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Aggregate Sentiment and Options Volume (Monthly & Yearly)\n",
    "# -------------------------------------\n",
    "\n",
    "# Monthly Aggregation\n",
    "monthly_sentiment = df.groupby(['ticker', pd.Grouper(key='Formatted_Date', freq='M')])['Sentiment_Score'].mean().reset_index()\n",
    "monthly_options_volume = df.groupby(['ticker', pd.Grouper(key='Formatted_Date', freq='M')])['Options Average Daily Volume'].mean().reset_index()\n",
    "\n",
    "# Yearly Aggregation\n",
    "yearly_sentiment = df.groupby(['ticker', pd.Grouper(key='Formatted_Date', freq='Y')])['Sentiment_Score'].mean().reset_index()\n",
    "yearly_options_volume = df.groupby(['ticker', pd.Grouper(key='Formatted_Date', freq='Y')])['Options Average Daily Volume'].mean().reset_index()\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Merge Sentiment with Options Volume\n",
    "# -------------------------------------\n",
    "\n",
    "# Monthly Merge\n",
    "monthly_merged = pd.merge(monthly_sentiment, monthly_options_volume, on=['ticker', 'Formatted_Date'], suffixes=('_sentiment', '_options'))\n",
    "\n",
    "# Yearly Merge\n",
    "yearly_merged = pd.merge(yearly_sentiment, yearly_options_volume, on=['ticker', 'Formatted_Date'], suffixes=('_sentiment', '_options'))\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Correlation Analysis\n",
    "# -------------------------------------\n",
    "\n",
    "def correlation_analysis(merged_df, level='Monthly'):\n",
    "    # Pearson Correlation\n",
    "    correlation, p_value = pearsonr(merged_df['Sentiment_Score'], merged_df['Options Average Daily Volume'])\n",
    "\n",
    "    print(f\"\\n📈 {level} Correlation between Sentiment Score and Options Average Daily Volume:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Monthly Correlation\n",
    "correlation_analysis(monthly_merged, 'Monthly')\n",
    "\n",
    "# Yearly Correlation\n",
    "correlation_analysis(yearly_merged, 'Yearly')\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Visualization of the Relationships\n",
    "# -------------------------------------\n",
    "\n",
    "def plot_correlation(merged_df, level='Monthly'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(\n",
    "        x='Sentiment_Score',\n",
    "        y='Options Average Daily Volume',\n",
    "        data=merged_df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(f'{level} Sentiment Score vs. Options Average Daily Volume')\n",
    "    plt.xlabel('Average Sentiment Score')\n",
    "    plt.ylabel('Options Average Daily Volume')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Monthly Data\n",
    "plot_correlation(monthly_merged, 'Monthly')\n",
    "\n",
    "# Plot for Yearly Data\n",
    "plot_correlation(yearly_merged, 'Yearly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Load the Dataset\n",
    "# -------------------------------------\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure proper datetime format and numeric types\n",
    "df['Formatted_Date'] = pd.to_datetime(df['Formatted_Date'], errors='coerce')\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "df['Reddit_Vader_Sentiment'] = pd.to_numeric(df['Reddit_Vader_Sentiment'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "df.dropna(subset=['Sentiment_Score', 'Reddit_Vader_Sentiment'], inplace=True)\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Filter Data for AAPL Only\n",
    "# -------------------------------------\n",
    "aapl_df = df[df['ticker'] == 'AAPL']\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Aggregate Sentiment and Reddit Vader Sentiment (Monthly & Yearly)\n",
    "# -------------------------------------\n",
    "\n",
    "# Monthly Aggregation for AAPL\n",
    "monthly_sentiment_aapl = aapl_df.groupby(pd.Grouper(key='Formatted_Date', freq='M'))['Sentiment_Score'].mean().reset_index()\n",
    "monthly_reddit_aapl = aapl_df.groupby(pd.Grouper(key='Formatted_Date', freq='M'))['Reddit_Vader_Sentiment'].mean().reset_index()\n",
    "\n",
    "# Yearly Aggregation for AAPL\n",
    "yearly_sentiment_aapl = aapl_df.groupby(pd.Grouper(key='Formatted_Date', freq='Y'))['Sentiment_Score'].mean().reset_index()\n",
    "yearly_reddit_aapl = aapl_df.groupby(pd.Grouper(key='Formatted_Date', freq='Y'))['Reddit_Vader_Sentiment'].mean().reset_index()\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Merge Sentiment with Reddit Vader Sentiment\n",
    "# -------------------------------------\n",
    "\n",
    "# Monthly Merge\n",
    "monthly_merged_aapl = pd.merge(monthly_sentiment_aapl, monthly_reddit_aapl, on='Formatted_Date', suffixes=('_news', '_reddit'))\n",
    "\n",
    "# Yearly Merge\n",
    "yearly_merged_aapl = pd.merge(yearly_sentiment_aapl, yearly_reddit_aapl, on='Formatted_Date', suffixes=('_news', '_reddit'))\n",
    "\n",
    "# Debug: Print column names to verify suffixes\n",
    "print(\"Monthly Merged Columns:\", monthly_merged_aapl.columns)\n",
    "print(\"Yearly Merged Columns:\", yearly_merged_aapl.columns)\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Correlation Analysis (with NaN/Inf Removal)\n",
    "# -------------------------------------\n",
    "\n",
    "def correlation_analysis(merged_df, level='Monthly'):\n",
    "    # Identify correct column names dynamically\n",
    "    sentiment_col = [col for col in merged_df.columns if 'Sentiment_Score' in col][0]\n",
    "    reddit_col = [col for col in merged_df.columns if 'Reddit_Vader_Sentiment' in col][0]\n",
    "\n",
    "    # Remove NaN and Infinite values\n",
    "    clean_df = merged_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[sentiment_col, reddit_col])\n",
    "\n",
    "    # Pearson Correlation\n",
    "    if not clean_df.empty:\n",
    "        correlation, p_value = pearsonr(clean_df[sentiment_col], clean_df[reddit_col])\n",
    "        print(f\"\\n📈 {level} Correlation between News Sentiment and Reddit Vader Sentiment for AAPL:\")\n",
    "        print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "        print(f\"P-value: {p_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {level} Correlation cannot be computed due to insufficient valid data.\")\n",
    "\n",
    "# Monthly Correlation for AAPL\n",
    "correlation_analysis(monthly_merged_aapl, 'Monthly')\n",
    "\n",
    "# Yearly Correlation for AAPL\n",
    "correlation_analysis(yearly_merged_aapl, 'Yearly')\n",
    "\n",
    "# -------------------------------------\n",
    "# 6. Visualization of the Relationships\n",
    "# -------------------------------------\n",
    "\n",
    "def plot_correlation(merged_df, level='Monthly'):\n",
    "    # Identify correct column names dynamically\n",
    "    sentiment_col = [col for col in merged_df.columns if 'Sentiment_Score' in col][0]\n",
    "    reddit_col = [col for col in merged_df.columns if 'Reddit_Vader_Sentiment' in col][0]\n",
    "\n",
    "    # Remove NaN and Infinite values for plotting\n",
    "    clean_df = merged_df.replace([np.inf, -np.inf], np.nan).dropna(subset=[sentiment_col, reddit_col])\n",
    "\n",
    "    if not clean_df.empty:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.regplot(\n",
    "            x=sentiment_col,\n",
    "            y=reddit_col,\n",
    "            data=clean_df,\n",
    "            scatter_kws={'alpha': 0.5},\n",
    "            line_kws={'color': 'red'}\n",
    "        )\n",
    "        plt.title(f'{level} News Sentiment vs. Reddit Vader Sentiment for AAPL')\n",
    "        plt.xlabel('Average News Sentiment Score')\n",
    "        plt.ylabel('Average Reddit Vader Sentiment')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\n⚠️ {level} plot cannot be generated due to insufficient valid data.\")\n",
    "\n",
    "# Plot for Monthly Data (AAPL)\n",
    "plot_correlation(monthly_merged_aapl, 'Monthly')\n",
    "\n",
    "# Plot for Yearly Data (AAPL)\n",
    "plot_correlation(yearly_merged_aapl, 'Yearly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalising Correlation Analysis to all tickers , after processing for just Apple above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure correct data types\n",
    "df['Formatted_Date'] = pd.to_datetime(df['Formatted_Date'], errors='coerce')\n",
    "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
    "df['Reddit_Vader_Sentiment'] = pd.to_numeric(df['Reddit_Vader_Sentiment'], errors='coerce')\n",
    "df['Daily Trading Volume'] = pd.to_numeric(df['Daily Trading Volume'], errors='coerce')\n",
    "df['Options Volume'] = pd.to_numeric(df['Options Volume'], errors='coerce')\n",
    "df['Options Average Daily Volume'] = pd.to_numeric(df['Options Average Daily Volume'], errors='coerce')\n",
    "df['Options % Spike'] = pd.to_numeric(df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Aggregate Data (Monthly & Yearly)\n",
    "# ---------------------------\n",
    "\n",
    "def aggregate_data(df, freq):\n",
    "    return df.groupby(['ticker', pd.Grouper(key='Formatted_Date', freq=freq)]).agg({\n",
    "        'Sentiment_Score': 'mean',\n",
    "        'Reddit_Vader_Sentiment': 'mean',\n",
    "        'Daily Trading Volume': 'sum',\n",
    "        'Options Volume': 'sum',\n",
    "        'Options Average Daily Volume': 'mean',\n",
    "        'Options % Spike': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "monthly_agg = aggregate_data(df, 'M')\n",
    "yearly_agg = aggregate_data(df, 'Y')\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Correlation Analysis\n",
    "# ---------------------------\n",
    "\n",
    "def correlation_analysis(agg_df, level):\n",
    "    correlation_pairs = [\n",
    "        ('Sentiment_Score', 'Options Volume'),\n",
    "        ('Sentiment_Score', 'Options Average Daily Volume'),\n",
    "        ('Sentiment_Score', 'Options % Spike'),\n",
    "\n",
    "        ('Reddit_Vader_Sentiment', 'Options Volume'),\n",
    "        ('Reddit_Vader_Sentiment', 'Options Average Daily Volume'),\n",
    "        ('Reddit_Vader_Sentiment', 'Options % Spike'),\n",
    "\n",
    "        ('Sentiment_Score', 'Reddit_Vader_Sentiment')\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n📈 {level} Correlation Analysis:\\n\" + \"-\"*40)\n",
    "\n",
    "    for pair in correlation_pairs:\n",
    "        x, y = pair\n",
    "        valid_data = agg_df[[x, y]].dropna()\n",
    "\n",
    "        if not valid_data.empty:\n",
    "            correlation, p_value = pearsonr(valid_data[x], valid_data[y])\n",
    "            print(f\"➡ Correlation between {x} and {y}: {correlation:.4f} (P-value: {p_value:.4f})\")\n",
    "        else:\n",
    "            print(f\"⚠ Not enough data for {x} and {y}\")\n",
    "\n",
    "# Monthly Correlation\n",
    "correlation_analysis(monthly_agg, 'Monthly')\n",
    "\n",
    "# Yearly Correlation\n",
    "correlation_analysis(yearly_agg, 'Yearly')\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Visualization\n",
    "# ---------------------------\n",
    "\n",
    "def plot_correlation(agg_df, x, y, level):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=x, y=y, data=agg_df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
    "    plt.title(f'{level} Correlation: {x} vs {y}')\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot: Monthly News Sentiment vs Options Volume\n",
    "plot_correlation(monthly_agg, 'Sentiment_Score', 'Options Volume', 'Monthly')\n",
    "\n",
    "# Plot: Yearly Reddit Sentiment vs Options % Spike\n",
    "plot_correlation(yearly_agg, 'Reddit_Vader_Sentiment', 'Options % Spike', 'Yearly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring lagged analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "\n",
    "# News articles with sentiment scores\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "\n",
    "# Missing dates with trading data\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Reddit_Vader_Sentiment'] = pd.to_numeric(articles_df['Reddit_Vader_Sentiment'], errors='coerce')\n",
    "missing_df['Options Volume'] = pd.to_numeric(missing_df['Options Volume'], errors='coerce')\n",
    "missing_df['Options Average Daily Volume'] = pd.to_numeric(missing_df['Options Average Daily Volume'], errors='coerce')\n",
    "missing_df['Options % Spike'] = pd.to_numeric(missing_df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Filter for AAPL\n",
    "articles_aapl = articles_df[articles_df['ticker'] == 'AAPL']\n",
    "missing_aapl = missing_df[missing_df['ticker'] == 'AAPL']\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Aggregate Sentiment by Date\n",
    "# -------------------------------\n",
    "\n",
    "# Aggregate news sentiment scores by date\n",
    "daily_sentiment = articles_aapl.groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "daily_sentiment.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Lag Sentiment by One Day\n",
    "# -------------------------------\n",
    "\n",
    "# Lag sentiment by one day to predict next day's trading\n",
    "daily_sentiment['Lagged_Sentiment'] = daily_sentiment['Avg_Sentiment'].shift(1)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Merge Sentiment with Trading Data\n",
    "# -------------------------------\n",
    "\n",
    "# Merge lagged sentiment with trading data\n",
    "merged_df = pd.merge(missing_aapl, daily_sentiment, how='left', on='Date')\n",
    "\n",
    "# Drop missing values after merge\n",
    "merged_df.dropna(subset=['Lagged_Sentiment', 'Options Volume', 'Options % Spike'], inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Correlation Analysis\n",
    "# -------------------------------\n",
    "\n",
    "def compute_correlation(x, y, label_x, label_y):\n",
    "    corr, p_value = pearsonr(merged_df[x], merged_df[y])\n",
    "    print(f\"📊 Correlation between {label_x} and {label_y}: {corr:.4f} (P-value: {p_value:.4f})\")\n",
    "    return corr\n",
    "\n",
    "# Correlation between lagged sentiment and trading activity\n",
    "compute_correlation('Lagged_Sentiment', 'Options Volume', 'Lagged Sentiment', 'Options Volume')\n",
    "compute_correlation('Lagged_Sentiment', 'Options Average Daily Volume', 'Lagged Sentiment', 'Options Average Daily Volume')\n",
    "compute_correlation('Lagged_Sentiment', 'Options % Spike', 'Lagged Sentiment', 'Options % Spike')\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Visualization\n",
    "# -------------------------------\n",
    "\n",
    "def plot_lagged_correlation(x, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=x, y=y, data=merged_df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Lagged Sentiment Score')\n",
    "    plt.ylabel(y)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot relationships\n",
    "plot_lagged_correlation('Lagged_Sentiment', 'Options Volume', 'Lagged Sentiment vs. Options Volume')\n",
    "plot_lagged_correlation('Lagged_Sentiment', 'Options % Spike', 'Lagged Sentiment vs. Options % Spike')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load datasets\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime format\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Options Volume'] = pd.to_numeric(articles_df['Options Volume'], errors='coerce')\n",
    "missing_df['Options Volume'] = pd.to_numeric(missing_df['Options Volume'], errors='coerce')\n",
    "\n",
    "# Filter for AAPL\n",
    "articles_aapl = articles_df[articles_df['ticker'] == 'AAPL']\n",
    "missing_aapl = missing_df[missing_df['ticker'] == 'AAPL']\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Merge Articles and Missing Data\n",
    "# -------------------------------\n",
    "\n",
    "# Aggregate daily sentiment scores\n",
    "articles_agg = articles_aapl.groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "articles_agg.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "# Merge with missing data to fill missing dates for options volume\n",
    "combined_df = pd.merge(missing_aapl[['Date', 'Options Volume']], articles_agg, how='outer', on='Date')\n",
    "combined_df['Options Volume'].fillna(0, inplace=True)  # Fill missing options volume with 0\n",
    "combined_df['Avg_Sentiment'].fillna(0, inplace=True)   # Fill missing sentiment with 0\n",
    "\n",
    "combined_df.sort_values('Date', inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Create Lagged Options Volume Columns\n",
    "# -------------------------------\n",
    "\n",
    "combined_df['Options_Volume_Lag_0'] = combined_df['Options Volume']\n",
    "combined_df['Options_Volume_Lag_1'] = combined_df['Options Volume'].shift(-1)\n",
    "combined_df['Options_Volume_Lag_3'] = combined_df['Options Volume'].shift(-3)\n",
    "combined_df['Options_Volume_Lag_5'] = combined_df['Options Volume'].shift(-5)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Correlation Analysis\n",
    "# -------------------------------\n",
    "\n",
    "def compute_correlation(x, y, label_x, label_y):\n",
    "    # Drop NaN values\n",
    "    clean_df = combined_df[[x, y]].dropna()\n",
    "\n",
    "    # Debug: Check valid data points\n",
    "    print(f\"\\n🔍 Valid data points for {label_x} vs {label_y}: {len(clean_df)}\")\n",
    "\n",
    "    if len(clean_df) < 2:\n",
    "        print(f\"⚠️ Not enough data to compute correlation between {label_x} and {label_y}. Skipping...\\n\")\n",
    "        return\n",
    "\n",
    "    # Pearson Correlation\n",
    "    correlation, p_value = pearsonr(clean_df[x], clean_df[y])\n",
    "\n",
    "    print(f\"\\n📊 Correlation between {label_x} and {label_y}:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Compute correlations for each lag\n",
    "compute_correlation('Avg_Sentiment', 'Options_Volume_Lag_0', 'Sentiment', 'Options Volume (Same Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Volume_Lag_1', 'Sentiment', 'Options Volume (+1 Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Volume_Lag_3', 'Sentiment', 'Options Volume (+3 Days)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Volume_Lag_5', 'Sentiment', 'Options Volume (+5 Days)')\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Visualization of Relationships\n",
    "# -------------------------------\n",
    "\n",
    "def plot_correlation(x, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=combined_df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('News Sentiment')\n",
    "    plt.ylabel('Options Trading Volume')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each lag\n",
    "plot_correlation('Avg_Sentiment', 'Options_Volume_Lag_0', 'Sentiment vs. Options Volume (Same Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Volume_Lag_1', 'Sentiment vs. Options Volume (+1 Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Volume_Lag_3', 'Sentiment vs. Options Volume (+3 Days)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Volume_Lag_5', 'Sentiment vs. Options Volume (+5 Days)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load datasets\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime format\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Options % Spike'] = pd.to_numeric(articles_df['Options % Spike'], errors='coerce')\n",
    "missing_df['Options % Spike'] = pd.to_numeric(missing_df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Take the absolute value of Options % Spike\n",
    "articles_df['Options % Spike'] = articles_df['Options % Spike'].abs()\n",
    "missing_df['Options % Spike'] = missing_df['Options % Spike'].abs()\n",
    "\n",
    "# Filter for AAPL\n",
    "articles_aapl = articles_df[articles_df['ticker'] == 'AAPL']\n",
    "missing_aapl = missing_df[missing_df['ticker'] == 'AAPL']\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Merge Articles and Missing Data\n",
    "# -------------------------------\n",
    "\n",
    "# Aggregate daily sentiment scores (excluding sentiment = 0)\n",
    "articles_agg = articles_aapl[articles_aapl['Sentiment_Score'] != 0].groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "articles_agg.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "# Merge with missing data to fill missing dates for Options % Spike\n",
    "combined_df = pd.merge(missing_aapl[['Date', 'Options % Spike']], articles_agg, how='outer', on='Date')\n",
    "combined_df['Options % Spike'].fillna(0, inplace=True)  # Fill missing % spike with 0\n",
    "combined_df['Avg_Sentiment'].fillna(0, inplace=True)    # Fill missing sentiment with 0\n",
    "\n",
    "# Drop rows where sentiment is 0\n",
    "combined_df = combined_df[combined_df['Avg_Sentiment'] != 0]\n",
    "\n",
    "combined_df.sort_values('Date', inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Create Lagged Absolute Options % Spike Columns\n",
    "# -------------------------------\n",
    "\n",
    "combined_df['Options_Spike_Lag_0'] = combined_df['Options % Spike']\n",
    "combined_df['Options_Spike_Lag_1'] = combined_df['Options % Spike'].shift(-1)\n",
    "combined_df['Options_Spike_Lag_3'] = combined_df['Options % Spike'].shift(-3)\n",
    "combined_df['Options_Spike_Lag_5'] = combined_df['Options % Spike'].shift(-5)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Correlation Analysis\n",
    "# -------------------------------\n",
    "\n",
    "def compute_correlation(x, y, label_x, label_y):\n",
    "    # Drop NaN values\n",
    "    clean_df = combined_df[[x, y]].dropna()\n",
    "\n",
    "    # Debug: Check valid data points\n",
    "    print(f\"\\n🔍 Valid data points for {label_x} vs {label_y}: {len(clean_df)}\")\n",
    "\n",
    "    if len(clean_df) < 2:\n",
    "        print(f\"⚠️ Not enough data to compute correlation between {label_x} and {label_y}. Skipping...\\n\")\n",
    "        return\n",
    "\n",
    "    # Pearson Correlation\n",
    "    correlation, p_value = pearsonr(clean_df[x], clean_df[y])\n",
    "\n",
    "    print(f\"\\n📊 Correlation between {label_x} and {label_y}:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Compute correlations for each lag\n",
    "compute_correlation('Avg_Sentiment', 'Options_Spike_Lag_0', 'Sentiment', 'Abs Options % Spike (Same Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Spike_Lag_1', 'Sentiment', 'Abs Options % Spike (+1 Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Spike_Lag_3', 'Sentiment', 'Abs Options % Spike (+3 Days)')\n",
    "compute_correlation('Avg_Sentiment', 'Options_Spike_Lag_5', 'Sentiment', 'Abs Options % Spike (+5 Days)')\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Visualization of Relationships\n",
    "# -------------------------------\n",
    "\n",
    "def plot_correlation(x, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=combined_df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('News Sentiment')\n",
    "    plt.ylabel('Absolute Options % Spike')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each lag\n",
    "plot_correlation('Avg_Sentiment', 'Options_Spike_Lag_0', 'Sentiment vs. Abs Options % Spike (Same Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Spike_Lag_1', 'Sentiment vs. Abs Options % Spike (+1 Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Spike_Lag_3', 'Sentiment vs. Abs Options % Spike (+3 Days)')\n",
    "plot_correlation('Avg_Sentiment', 'Options_Spike_Lag_5', 'Sentiment vs. Abs Options % Spike (+5 Days)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load datasets\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime format\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Options % Spike'] = pd.to_numeric(articles_df['Options % Spike'], errors='coerce')\n",
    "missing_df['Options % Spike'] = pd.to_numeric(missing_df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Compute log of the absolute value of Options % Spike (+1 to avoid log(0))\n",
    "articles_df['Log_Options_%_Spike'] = np.log1p(articles_df['Options % Spike'].abs())\n",
    "missing_df['Log_Options_%_Spike'] = np.log1p(missing_df['Options % Spike'].abs())\n",
    "\n",
    "# Filter for AAPL\n",
    "articles_aapl = articles_df[articles_df['ticker'] == 'AAPL']\n",
    "missing_aapl = missing_df[missing_df['ticker'] == 'AAPL']\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Merge Articles and Missing Data\n",
    "# -------------------------------\n",
    "\n",
    "# Aggregate daily sentiment scores (excluding sentiment = 0)\n",
    "articles_agg = articles_aapl[articles_aapl['Sentiment_Score'] != 0].groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "articles_agg.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "# Merge with missing data to fill missing dates for Options % Spike\n",
    "combined_df = pd.merge(missing_aapl[['Date', 'Log_Options_%_Spike']], articles_agg, how='outer', on='Date')\n",
    "combined_df['Log_Options_%_Spike'].fillna(0, inplace=True)\n",
    "combined_df['Avg_Sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "# Drop rows where sentiment is 0\n",
    "combined_df = combined_df[combined_df['Avg_Sentiment'] != 0]\n",
    "\n",
    "combined_df.sort_values('Date', inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Create Lagged Log Options % Spike Columns\n",
    "# -------------------------------\n",
    "\n",
    "combined_df['Log_Spike_Lag_0'] = combined_df['Log_Options_%_Spike']\n",
    "combined_df['Log_Spike_Lag_1'] = combined_df['Log_Options_%_Spike'].shift(-1)\n",
    "combined_df['Log_Spike_Lag_3'] = combined_df['Log_Options_%_Spike'].shift(-3)\n",
    "combined_df['Log_Spike_Lag_5'] = combined_df['Log_Options_%_Spike'].shift(-5)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Correlation Analysis\n",
    "# -------------------------------\n",
    "\n",
    "def compute_correlation(x, y, label_x, label_y):\n",
    "    # Drop NaN values\n",
    "    clean_df = combined_df[[x, y]].dropna()\n",
    "\n",
    "    # Debug: Check valid data points\n",
    "    print(f\"\\n🔍 Valid data points for {label_x} vs {label_y}: {len(clean_df)}\")\n",
    "\n",
    "    if len(clean_df) < 2:\n",
    "        print(f\"⚠️ Not enough data to compute correlation between {label_x} and {label_y}. Skipping...\\n\")\n",
    "        return\n",
    "\n",
    "    # Pearson Correlation\n",
    "    correlation, p_value = pearsonr(clean_df[x], clean_df[y])\n",
    "\n",
    "    print(f\"\\n📊 Correlation between {label_x} and {label_y}:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Compute correlations for each lag\n",
    "compute_correlation('Avg_Sentiment', 'Log_Spike_Lag_0', 'Sentiment', 'Log Abs Options % Spike (Same Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Log_Spike_Lag_1', 'Sentiment', 'Log Abs Options % Spike (+1 Day)')\n",
    "compute_correlation('Avg_Sentiment', 'Log_Spike_Lag_3', 'Sentiment', 'Log Abs Options % Spike (+3 Days)')\n",
    "compute_correlation('Avg_Sentiment', 'Log_Spike_Lag_5', 'Sentiment', 'Log Abs Options % Spike (+5 Days)')\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Visualization of Relationships\n",
    "# -------------------------------\n",
    "\n",
    "def plot_correlation(x, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=combined_df,\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('News Sentiment')\n",
    "    plt.ylabel('Log of Absolute Options % Spike')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each lag\n",
    "plot_correlation('Avg_Sentiment', 'Log_Spike_Lag_0', 'Sentiment vs. Log Abs Options % Spike (Same Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Log_Spike_Lag_1', 'Sentiment vs. Log Abs Options % Spike (+1 Day)')\n",
    "plot_correlation('Avg_Sentiment', 'Log_Spike_Lag_3', 'Sentiment vs. Log Abs Options % Spike (+3 Days)')\n",
    "plot_correlation('Avg_Sentiment', 'Log_Spike_Lag_5', 'Sentiment vs. Log Abs Options % Spike (+5 Days)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Identifying tickers with highest correlation to explore industry trends\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load datasets\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime format\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Options % Spike'] = pd.to_numeric(articles_df['Options % Spike'], errors='coerce')\n",
    "missing_df['Options % Spike'] = pd.to_numeric(missing_df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Compute log of the absolute value of Options % Spike (+1 to avoid log(0))\n",
    "articles_df['Log_Options_%_Spike'] = np.log1p(articles_df['Options % Spike'].abs())\n",
    "missing_df['Log_Options_%_Spike'] = np.log1p(missing_df['Options % Spike'].abs())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Analysis Across All Tickers\n",
    "# -------------------------------\n",
    "\n",
    "# Initialize dictionary to store correlation results\n",
    "correlation_results = []\n",
    "\n",
    "# Get the list of unique tickers\n",
    "tickers = articles_df['ticker'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Filter data for each ticker\n",
    "    articles_ticker = articles_df[articles_df['ticker'] == ticker]\n",
    "    missing_ticker = missing_df[missing_df['ticker'] == ticker]\n",
    "\n",
    "    # Aggregate daily sentiment scores (excluding sentiment = 0)\n",
    "    articles_agg = articles_ticker[articles_ticker['Sentiment_Score'] != 0].groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "    articles_agg.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "    # Merge with missing data to fill missing dates for Options % Spike\n",
    "    combined_df = pd.merge(missing_ticker[['Date', 'Log_Options_%_Spike']], articles_agg, how='outer', on='Date')\n",
    "    combined_df['Log_Options_%_Spike'].fillna(0, inplace=True)\n",
    "    combined_df['Avg_Sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop rows where sentiment is 0\n",
    "    combined_df = combined_df[combined_df['Avg_Sentiment'] != 0]\n",
    "\n",
    "    combined_df.sort_values('Date', inplace=True)\n",
    "\n",
    "    # Create lagged columns for Options % Spike\n",
    "    combined_df['Log_Spike_Lag_0'] = combined_df['Log_Options_%_Spike']\n",
    "    combined_df['Log_Spike_Lag_1'] = combined_df['Log_Options_%_Spike'].shift(-1)\n",
    "    combined_df['Log_Spike_Lag_3'] = combined_df['Log_Options_%_Spike'].shift(-3)\n",
    "    combined_df['Log_Spike_Lag_5'] = combined_df['Log_Options_%_Spike'].shift(-5)\n",
    "\n",
    "    # Function to compute correlation\n",
    "    def compute_corr(x, y):\n",
    "        clean_df = combined_df[[x, y]].dropna()\n",
    "        if len(clean_df) < 2:\n",
    "            return np.nan\n",
    "        correlation, _ = pearsonr(clean_df[x], clean_df[y])\n",
    "        return correlation\n",
    "\n",
    "    # Compute correlations for all lag periods\n",
    "    corr_lag_0 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_0')\n",
    "    corr_lag_1 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_1')\n",
    "    corr_lag_3 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_3')\n",
    "    corr_lag_5 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_5')\n",
    "\n",
    "    # Store results\n",
    "    correlation_results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Corr_Lag_0': corr_lag_0,\n",
    "        'Corr_Lag_1': corr_lag_1,\n",
    "        'Corr_Lag_3': corr_lag_3,\n",
    "        'Corr_Lag_5': corr_lag_5\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Identify Tickers with Highest Correlation\n",
    "# -------------------------------\n",
    "\n",
    "# Find the ticker with the highest correlation for each lag\n",
    "max_corr_lag_0 = results_df.loc[results_df['Corr_Lag_0'].idxmax()]\n",
    "max_corr_lag_1 = results_df.loc[results_df['Corr_Lag_1'].idxmax()]\n",
    "max_corr_lag_3 = results_df.loc[results_df['Corr_Lag_3'].idxmax()]\n",
    "max_corr_lag_5 = results_df.loc[results_df['Corr_Lag_5'].idxmax()]\n",
    "\n",
    "print(\"\\n📊 **Tickers with Highest Correlations** 📊\")\n",
    "print(f\"\\nSame Day (Lag 0): {max_corr_lag_0['Ticker']} with correlation {max_corr_lag_0['Corr_Lag_0']:.4f}\")\n",
    "print(f\"+1 Day (Lag 1): {max_corr_lag_1['Ticker']} with correlation {max_corr_lag_1['Corr_Lag_1']:.4f}\")\n",
    "print(f\"+3 Days (Lag 3): {max_corr_lag_3['Ticker']} with correlation {max_corr_lag_3['Corr_Lag_3']:.4f}\")\n",
    "print(f\"+5 Days (Lag 5): {max_corr_lag_5['Ticker']} with correlation {max_corr_lag_5['Corr_Lag_5']:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save Results to CSV\n",
    "# -------------------------------\n",
    "\n",
    "results_df.to_csv(\"/content/drive/MyDrive/FYP/correlation_results_all_tickers.csv\", index=False)\n",
    "print(\"\\n✅ Correlation results saved to 'correlation_results_all_tickers.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Exploring which tickers exhibit greatest correlation over different time lags\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Datasets\n",
    "# -------------------------------\n",
    "articles_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "missing_data_path = \"/content/drive/MyDrive/FYP/missing_dates_merged_articles_with_options_data.csv\"\n",
    "\n",
    "# Load datasets\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "missing_df = pd.read_csv(missing_data_path)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning\n",
    "# -------------------------------\n",
    "\n",
    "# Convert dates to datetime format\n",
    "articles_df['Formatted_Date'] = pd.to_datetime(articles_df['Formatted_Date'], errors='coerce')\n",
    "missing_df['Date'] = pd.to_datetime(missing_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "articles_df['Sentiment_Score'] = pd.to_numeric(articles_df['Sentiment_Score'], errors='coerce')\n",
    "articles_df['Options % Spike'] = pd.to_numeric(articles_df['Options % Spike'], errors='coerce')\n",
    "missing_df['Options % Spike'] = pd.to_numeric(missing_df['Options % Spike'], errors='coerce')\n",
    "\n",
    "# Compute log of the absolute value of Options % Spike (+1 to avoid log(0))\n",
    "articles_df['Log_Options_%_Spike'] = np.log1p(articles_df['Options % Spike'].abs())\n",
    "missing_df['Log_Options_%_Spike'] = np.log1p(missing_df['Options % Spike'].abs())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Correlation Analysis Across All Tickers\n",
    "# -------------------------------\n",
    "\n",
    "# Initialize dictionary to store correlation results\n",
    "correlation_results = []\n",
    "\n",
    "# Get the list of unique tickers\n",
    "tickers = articles_df['ticker'].unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Filter data for each ticker\n",
    "    articles_ticker = articles_df[articles_df['ticker'] == ticker]\n",
    "    missing_ticker = missing_df[missing_df['ticker'] == ticker]\n",
    "\n",
    "    # Aggregate daily sentiment scores (excluding sentiment = 0)\n",
    "    articles_agg = articles_ticker[articles_ticker['Sentiment_Score'] != 0].groupby('Formatted_Date')['Sentiment_Score'].mean().reset_index()\n",
    "    articles_agg.rename(columns={'Formatted_Date': 'Date', 'Sentiment_Score': 'Avg_Sentiment'}, inplace=True)\n",
    "\n",
    "    # Merge with missing data to fill missing dates for Options % Spike\n",
    "    combined_df = pd.merge(missing_ticker[['Date', 'Log_Options_%_Spike']], articles_agg, how='outer', on='Date')\n",
    "    combined_df['Log_Options_%_Spike'].fillna(0, inplace=True)\n",
    "    combined_df['Avg_Sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop rows where sentiment is 0\n",
    "    combined_df = combined_df[combined_df['Avg_Sentiment'] != 0]\n",
    "\n",
    "    combined_df.sort_values('Date', inplace=True)\n",
    "\n",
    "    # Create lagged columns for Options % Spike\n",
    "    combined_df['Log_Spike_Lag_0'] = combined_df['Log_Options_%_Spike']\n",
    "    combined_df['Log_Spike_Lag_1'] = combined_df['Log_Options_%_Spike'].shift(-1)\n",
    "    combined_df['Log_Spike_Lag_3'] = combined_df['Log_Options_%_Spike'].shift(-3)\n",
    "    combined_df['Log_Spike_Lag_5'] = combined_df['Log_Options_%_Spike'].shift(-5)\n",
    "\n",
    "    # Function to compute correlation\n",
    "    def compute_corr(x, y):\n",
    "        clean_df = combined_df[[x, y]].dropna()\n",
    "        if len(clean_df) < 2:\n",
    "            return np.nan\n",
    "        correlation, _ = pearsonr(clean_df[x], clean_df[y])\n",
    "        return correlation\n",
    "\n",
    "    # Compute correlations for all lag periods\n",
    "    corr_lag_0 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_0')\n",
    "    corr_lag_1 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_1')\n",
    "    corr_lag_3 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_3')\n",
    "    corr_lag_5 = compute_corr('Avg_Sentiment', 'Log_Spike_Lag_5')\n",
    "\n",
    "    # Store results\n",
    "    correlation_results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Corr_Lag_0': corr_lag_0,\n",
    "        'Corr_Lag_1': corr_lag_1,\n",
    "        'Corr_Lag_3': corr_lag_3,\n",
    "        'Corr_Lag_5': corr_lag_5\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Identify Tickers with Highest Correlation\n",
    "# -------------------------------\n",
    "\n",
    "# Find the ticker with the highest correlation for each lag\n",
    "max_corr_lag_0 = results_df.loc[results_df['Corr_Lag_0'].idxmax()]\n",
    "max_corr_lag_1 = results_df.loc[results_df['Corr_Lag_1'].idxmax()]\n",
    "max_corr_lag_3 = results_df.loc[results_df['Corr_Lag_3'].idxmax()]\n",
    "max_corr_lag_5 = results_df.loc[results_df['Corr_Lag_5'].idxmax()]\n",
    "\n",
    "print(\"\\n📊 **Tickers with Highest Correlations** 📊\")\n",
    "print(f\"\\nSame Day (Lag 0): {max_corr_lag_0['Ticker']} with correlation {max_corr_lag_0['Corr_Lag_0']:.4f}\")\n",
    "print(f\"+1 Day (Lag 1): {max_corr_lag_1['Ticker']} with correlation {max_corr_lag_1['Corr_Lag_1']:.4f}\")\n",
    "print(f\"+3 Days (Lag 3): {max_corr_lag_3['Ticker']} with correlation {max_corr_lag_3['Corr_Lag_3']:.4f}\")\n",
    "print(f\"+5 Days (Lag 5): {max_corr_lag_5['Ticker']} with correlation {max_corr_lag_5['Corr_Lag_5']:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save Results to CSV\n",
    "# -------------------------------\n",
    "\n",
    "results_df.to_csv(\"/content/drive/MyDrive/FYP/correlation_results_all_tickers.csv\", index=False)\n",
    "print(\"\\n✅ Correlation results saved to 'correlation_results_all_tickers.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Better visualisation of all correlations available \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load the Dataset\n",
    "# ----------------------------\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_articles_with_reddit_sentiment_all.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Data Preprocessing\n",
    "# ----------------------------\n",
    "# Ensure numeric data types\n",
    "numeric_columns = [\n",
    "    'Sentiment_Score',\n",
    "    'Reddit_Vader_Sentiment',\n",
    "    'Options Volume',\n",
    "    'Options % Spike',\n",
    "    'Trading Volume',\n",
    "    '% Spike'\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "df.dropna(subset=numeric_columns + ['ticker'], inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Spearman Correlation Calculation\n",
    "# ----------------------------\n",
    "def calculate_spearman(df, metric):\n",
    "    correlations = {}\n",
    "    for ticker in df['ticker'].unique():\n",
    "        subset = df[df['ticker'] == ticker]\n",
    "        if len(subset) > 1:  # Need at least 2 data points\n",
    "            corr, _ = spearmanr(subset['Sentiment_Score'], subset[metric])\n",
    "            correlations[ticker] = corr\n",
    "        else:\n",
    "            correlations[ticker] = None\n",
    "    return correlations\n",
    "\n",
    "# Compute correlations for all 5 metrics\n",
    "metrics = {\n",
    "    \"Reddit_Vader_Sentiment\": \"Sentiment vs Reddit Vader Sentiment\",\n",
    "    \"Options Volume\": \"Sentiment vs Options Trading Volume\",\n",
    "    \"Options % Spike\": \"Sentiment vs Options % Spike\",\n",
    "    \"Trading Volume\": \"Sentiment vs Trading Volume\",\n",
    "    \"% Spike\": \"Sentiment vs % Spike\"\n",
    "}\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "for metric, label in metrics.items():\n",
    "    correlation_results[label] = calculate_spearman(df, metric)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Visualization\n",
    "# ----------------------------\n",
    "def plot_horizontal_bars(correlations, title):\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    corr_df = pd.DataFrame(list(correlations.items()), columns=['Ticker', 'Spearman_Correlation'])\n",
    "    corr_df.dropna(inplace=True)\n",
    "    corr_df.sort_values('Spearman_Correlation', inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Spearman_Correlation', y='Ticker', data=corr_df, palette='viridis')\n",
    "    plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "    plt.title(f'{title}')\n",
    "    plt.xlabel('Spearman Rank Correlation')\n",
    "    plt.ylabel('Ticker')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all five charts\n",
    "for label, correlations in correlation_results.items():\n",
    "    plot_horizontal_bars(correlations, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Exploring consistently negative correlations - why is this the case? \n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Identify Consistently Negative Correlations\n",
    "# ----------------------------\n",
    "\n",
    "def find_consistently_negative(correlation_results):\n",
    "    # Convert the correlation dictionaries to DataFrames\n",
    "    combined_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "    # Drop rows with NaN values (for missing correlations)\n",
    "    combined_df.dropna(inplace=True)\n",
    "\n",
    "    # Identify tickers where all correlations are negative across all metrics\n",
    "    negative_tickers = combined_df[(combined_df < 0).all(axis=1)].index.tolist()\n",
    "\n",
    "    return combined_df, negative_tickers\n",
    "\n",
    "# Find companies with negative correlations in all metrics\n",
    "combined_correlation_df, consistently_negative_tickers = find_consistently_negative(correlation_results)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Display Results\n",
    "# ----------------------------\n",
    "print(\"\\n📉 **Companies with Consistently Negative Correlation Across All Metrics:**\")\n",
    "for ticker in consistently_negative_tickers:\n",
    "    print(f\" - {ticker}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Optional: Export Results to CSV\n",
    "# ----------------------------\n",
    "# Save the full correlation data with negative indicators\n",
    "combined_correlation_df.to_csv(\"/content/drive/MyDrive/FYP/consistently_negative_correlation_tickers.csv\", index=True)\n",
    "print(\"\\n📂 Results exported to 'consistently_negative_correlation_tickers.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Looking into companies with mixed (i.e. positive correlation over one time lag period but negative over another)\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load the Correlation Data\n",
    "# --------------------------\n",
    "file_path = \"/content/drive/MyDrive/FYP/filtered_correlation_analysis_with_avg.csv\"  # Update if needed\n",
    "correlations_df = pd.read_csv(file_path)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Identify Companies with Mixed Correlations\n",
    "# --------------------------\n",
    "def has_mixed_correlation(row):\n",
    "    values = [row['Sentiment_vs_Reddit'], row['Sentiment_vs_Options_Spike'], row['Sentiment_vs_Trading_Spike']]\n",
    "    return any(v > 0 for v in values) and any(v < 0 for v in values)\n",
    "\n",
    "# Apply the function to flag mixed correlations\n",
    "correlations_df['Mixed_Correlation'] = correlations_df.apply(has_mixed_correlation, axis=1)\n",
    "\n",
    "# Filter companies with mixed correlations\n",
    "mixed_correlation_companies = correlations_df[correlations_df['Mixed_Correlation'] == True]\n",
    "\n",
    "# --------------------------\n",
    "# 3. Display and Save Results\n",
    "# --------------------------\n",
    "print(\"📊 **Companies with Mixed Correlations Across Metrics:**\")\n",
    "print(mixed_correlation_companies[['ticker', 'Sentiment_vs_Reddit', 'Sentiment_vs_Options_Spike', 'Sentiment_vs_Trading_Spike']])\n",
    "\n",
    "# Save the results to a CSV\n",
    "mixed_correlation_companies.to_csv(\"/content/drive/MyDrive/FYP/mixed_correlation_companies.csv\", index=False)\n",
    "print(\"\\n✅ Results saved to 'mixed_correlation_companies.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Granger Casuality tests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the date column is properly formatted\n",
    "df['Formatted_Date'] = pd.to_datetime(df['Formatted_Date'])\n",
    "\n",
    "# Prepare results dictionary to store outcomes for all tickers\n",
    "results_dict = {}\n",
    "\n",
    "# Set maximum lag for Granger causality\n",
    "max_lag = 3\n",
    "\n",
    "# Iterate over all unique tickers in the dataset\n",
    "all_tickers = df['ticker'].unique()\n",
    "\n",
    "for ticker in all_tickers:\n",
    "    # Filter the data for the current ticker and sort by date\n",
    "    df_ticker = df[df['ticker'] == ticker].sort_values(\"Formatted_Date\")\n",
    "\n",
    "    # Drop rows with NaN or infinite values\n",
    "    df_ticker = df_ticker.replace([np.inf, -np.inf], np.nan).dropna(subset=['Sentiment_Score', 'Reddit_Vader_Sentiment'])\n",
    "\n",
    "    # Ensure there are enough data points for Granger causality\n",
    "    if len(df_ticker) < max_lag + 1:\n",
    "        print(f\"Skipping {ticker}: Not enough data points.\")\n",
    "        continue\n",
    "\n",
    "    # Extract the two time series\n",
    "    news_sentiment = df_ticker['Sentiment_Score'].values\n",
    "    reddit_sentiment = df_ticker['Reddit_Vader_Sentiment'].values\n",
    "\n",
    "    # Construct arrays for \"news -> Reddit\" Granger causality\n",
    "    data_news_to_reddit = np.column_stack([reddit_sentiment, news_sentiment])\n",
    "\n",
    "    # Run Granger causality tests for \"news -> Reddit\"\n",
    "    try:\n",
    "        result_n2r = grangercausalitytests(data_news_to_reddit, maxlag=max_lag, verbose=False)\n",
    "        pvals_n2r = {lag: result_n2r[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker} for news -> Reddit: {e}\")\n",
    "        pvals_n2r = {lag: None for lag in range(1, max_lag + 1)}\n",
    "\n",
    "    # Construct arrays for \"Reddit -> news\" Granger causality\n",
    "    data_reddit_to_news = np.column_stack([news_sentiment, reddit_sentiment])\n",
    "\n",
    "    # Run Granger causality tests for \"Reddit -> news\"\n",
    "    try:\n",
    "        result_r2n = grangercausalitytests(data_reddit_to_news, maxlag=max_lag, verbose=False)\n",
    "        pvals_r2n = {lag: result_r2n[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker} for Reddit -> news: {e}\")\n",
    "        pvals_r2n = {lag: None for lag in range(1, max_lag + 1)}\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    results_dict[ticker] = {\n",
    "        \"News_Granger_Causes_Reddit\": pvals_n2r,\n",
    "        \"Reddit_Granger_Causes_News\": pvals_r2n\n",
    "    }\n",
    "\n",
    "# Convert the results dictionary into a DataFrame for easier analysis and export\n",
    "rows = []\n",
    "for ticker, outcomes in results_dict.items():\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        rows.append({\n",
    "            \"Ticker\": ticker,\n",
    "            \"Lag\": lag,\n",
    "            \"News_Granger_Causes_Reddit_pval\": outcomes[\"News_Granger_Causes_Reddit\"].get(lag),\n",
    "            \"Reddit_Granger_Causes_News_pval\": outcomes[\"Reddit_Granger_Causes_News\"].get(lag)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save results to a CSV\n",
    "output_path = \"granger_causality_results.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Granger causality results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load the dataset (replace with your file path)\n",
    "file_path = \"/content/drive/MyDrive/FYP/v2_reddit_sentiment_all_with_vader.csv\"\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Ensure the date column is in datetime format and sort the data\n",
    "data['Formatted_Date'] = pd.to_datetime(data['Formatted_Date'])\n",
    "data = data.sort_values(by=['ticker', 'Formatted_Date'])\n",
    "\n",
    "# Focus on relevant columns\n",
    "data = data[['ticker', 'Formatted_Date', 'Sentiment_Score', 'Reddit_Vader_Sentiment']]\n",
    "\n",
    "# List of tickers to process\n",
    "tickers = data['ticker'].unique()\n",
    "\n",
    "# Define maximum lag\n",
    "max_lag = 3\n",
    "\n",
    "# Initialize a dictionary to store Granger causality p-values\n",
    "granger_results = []\n",
    "\n",
    "# Iterate through each ticker\n",
    "for ticker in tickers:\n",
    "    # Filter data for the current ticker\n",
    "    ticker_data = data[data['ticker'] == ticker].dropna()\n",
    "    if len(ticker_data) < max_lag + 1:\n",
    "        continue  # Skip if not enough data points\n",
    "\n",
    "    # Prepare the time series data\n",
    "    news_sentiment = ticker_data['Sentiment_Score'].values\n",
    "    reddit_sentiment = ticker_data['Reddit_Vader_Sentiment'].values\n",
    "\n",
    "    # Create a 2D array for Granger causality tests\n",
    "    data_news_to_reddit = np.column_stack([reddit_sentiment, news_sentiment])\n",
    "\n",
    "    # Perform Granger causality tests for lags 1 to max_lag\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        try:\n",
    "            test_result = grangercausalitytests(data_news_to_reddit, maxlag=lag, verbose=False)\n",
    "            p_value = test_result[lag][0]['ssr_ftest'][1]  # Extract p-value for the F-test\n",
    "            granger_results.append({'Ticker': ticker, 'Lag': lag, 'p-value': p_value})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker} at lag {lag}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "granger_df = pd.DataFrame(granger_results)\n",
    "\n",
    "# Pivot the DataFrame for visualization\n",
    "pivot_df = granger_df.pivot(index='Ticker', columns='Lag', values='p-value')\n",
    "\n",
    "# Drop tickers that don't have values for all lags\n",
    "pivot_df = pivot_df.dropna()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, len(pivot_df) * 0.5))\n",
    "sns.heatmap(\n",
    "    pivot_df,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap='RdYlGn_r',\n",
    "    cbar_kws={'label': 'p-value'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    mask=pivot_df.isna(),\n",
    "    center=0.05\n",
    ")\n",
    "plt.title(\"Granger Causality p-values for News Sentiment → Reddit Sentiment\", fontsize=14, pad=20)\n",
    "plt.xlabel(\"Lag (days)\", fontsize=12)\n",
    "plt.ylabel(\"Ticker\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"granger_causality_news_reddit_filtered.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save the filtered data to a CSV for reference\n",
    "pivot_df.to_csv(\"filtered_granger_causality_results.csv\")\n",
    "print(\"Filtered Granger causality data saved to filtered_granger_causality_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Summary sector wise distinctions - for thesis graphics \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"ticker_optimal_weights.csv\"  # Update this path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = ['ticker', 'best_correlation']\n",
    "if not all(col in data.columns for col in required_columns):\n",
    "    raise ValueError(\"CSV file is missing one or more required columns: \" + \", \".join(required_columns))\n",
    "\n",
    "# Create a mapping of tickers to sectors (example mapping)\n",
    "ticker_to_sector = {\n",
    "    # Communication Services\n",
    "    'DIS': 'Communication Services', 'NFLX': 'Communication Services', 'CMCSA': 'Communication Services',\n",
    "    'TMUS': 'Communication Services', 'GOOGL': 'Communication Services', 'CHTR': 'Communication Services',\n",
    "\n",
    "    # Consumer Discretionary\n",
    "    'AMZN': 'Consumer Discretionary', 'TSLA': 'Consumer Discretionary', 'NKE': 'Consumer Discretionary',\n",
    "    'HD': 'Consumer Discretionary', 'AZO': 'Consumer Discretionary', 'YUM': 'Consumer Discretionary',\n",
    "\n",
    "    # Consumer Staples\n",
    "    'WMT': 'Consumer Staples', 'KO': 'Consumer Staples', 'PEP': 'Consumer Staples', 'MDLZ': 'Consumer Staples',\n",
    "    'PG': 'Consumer Staples', 'KHC': 'Consumer Staples', 'MO': 'Consumer Staples',\n",
    "\n",
    "    # Energy\n",
    "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'NEM': 'Energy',\n",
    "\n",
    "    # Financials\n",
    "    'BAC': 'Financials', 'JPM': 'Financials', 'C': 'Financials', 'MA': 'Financials', 'V': 'Financials',\n",
    "    'WFC': 'Financials', 'CB': 'Financials',\n",
    "\n",
    "    # Health Care\n",
    "    'JNJ': 'Health Care', 'PFE': 'Health Care', 'ABBV': 'Health Care', 'MRK': 'Health Care',\n",
    "    'LLY': 'Health Care', 'GILD': 'Health Care', 'MDT': 'Health Care', 'UNH': 'Health Care',\n",
    "\n",
    "    # Industrials\n",
    "    'GE': 'Industrials', 'BA': 'Industrials', 'UPS': 'Industrials', 'MMM': 'Industrials',\n",
    "    'DELL': 'Industrials',\n",
    "\n",
    "    # Information Technology\n",
    "    'AAPL': 'Information Technology', 'MSFT': 'Information Technology', 'NVDA': 'Information Technology',\n",
    "    'QCOM': 'Information Technology', 'ADBE': 'Information Technology', 'INTC': 'Information Technology',\n",
    "    'CSCO': 'Information Technology', 'TXN': 'Information Technology', 'IBM': 'Information Technology',\n",
    "    'ORCL': 'Information Technology', 'PYPL': 'Information Technology', 'AVGO': 'Information Technology',\n",
    "    'AMD': 'Information Technology', 'ADP': 'Information Technology', 'INTU': 'Information Technology',\n",
    "\n",
    "    # Materials\n",
    "    'LIN': 'Materials', 'DHR': 'Materials',\n",
    "\n",
    "    # Real Estate\n",
    "    'PSA': 'Real Estate',\n",
    "\n",
    "    # Utilities\n",
    "    'DUK': 'Utilities', 'ED': 'Utilities', 'NEE': 'Utilities', 'PPL': 'Utilities',\n",
    "}\n",
    "\n",
    "# Map the sector information to the data\n",
    "data['sector'] = data['ticker'].map(ticker_to_sector)\n",
    "\n",
    "# Drop rows where the sector is missing\n",
    "data = data.dropna(subset=['sector'])\n",
    "\n",
    "# Filter out invalid correlation values\n",
    "data = data[data['best_correlation'].notna() & (data['best_correlation'] > -float('inf'))]\n",
    "\n",
    "# Calculate median and IQR for each sector\n",
    "sector_stats = (\n",
    "    data.groupby(\"sector\")[\"best_correlation\"]\n",
    "    .agg(median=\"median\", iqr=lambda x: x.quantile(0.75) - x.quantile(0.25))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort sectors by IQR (descending)\n",
    "sorted_sectors = sector_stats.sort_values(by=\"iqr\", ascending=False)[\"sector\"]\n",
    "data[\"sector\"] = pd.Categorical(data[\"sector\"], categories=sorted_sectors, ordered=True)\n",
    "\n",
    "# Plot settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure with two subplots: one for the histogram and one for the boxplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Histogram of correlation values grouped by sector\n",
    "sns.histplot(\n",
    "    data=data,\n",
    "    x='best_correlation',\n",
    "    hue='sector',\n",
    "    multiple='stack',\n",
    "    kde=False,\n",
    "    palette='tab10',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Histogram of Correlation Distribution by Sector\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Correlation with Financial News Sentiment\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Count\", fontsize=12)\n",
    "axes[0].legend(title='Sector', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Boxplot of correlation values grouped by sector\n",
    "sns.boxplot(\n",
    "    data=data,\n",
    "    x='sector',\n",
    "    y='best_correlation',\n",
    "    palette='muted',\n",
    "    ax=axes[1]\n",
    ")\n",
    "\n",
    "# Add median values to the boxplot\n",
    "medians = data.groupby(\"sector\")[\"best_correlation\"].median()\n",
    "for i, median in enumerate(medians):\n",
    "    axes[1].text(\n",
    "        i, median, f\"{median:.2f}\", ha=\"center\", va=\"center\",\n",
    "        color=\"white\", fontweight=\"bold\", fontsize=9,\n",
    "        bbox=dict(facecolor=\"black\", alpha=0.7)\n",
    "    )\n",
    "\n",
    "axes[1].set_title(\"Boxplot of Correlation Distribution by Sector\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Sector (Sorted by IQR)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Correlation with Financial News Sentiment\", fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
